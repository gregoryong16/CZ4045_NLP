{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1381fbead958f0e2\n",
      "Reusing dataset csv (C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014cebec1eb64ac0b8544a14793f219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = './data/'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': base_url+'bert_train.csv','validation': base_url+'bert_val.csv','test': base_url+'bert_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â„¢', 'âœ…', 'ðŸ’—', 'ðŸ˜¢', 'ðŸ³ï¸\\u200dðŸŒˆ', 'ðŸ˜Œ', 'ðŸ¤¦\\u200dâ™€ï¸', 'ðŸ³', 'ðŸ’€', 'ðŸŒˆ', 'ðŸ‘ðŸ»', 'âœ¨', 'ðŸŒŸ', 'ðŸŒ¹', 'ðŸ˜•', 'â­', 'ðŸ˜…', 'ðŸ™', 'ðŸ’', 'ðŸ§', 'ðŸ˜­', 'ðŸ™„', 'ðŸ–¤', 'ðŸ¤§', 'ðŸ¤£', 'ðŸ—ƒï¸', 'ðŸ—“ï¸', 'âž¡ï¸', 'ðŸ”»', 'ðŸ¤¤', 'ðŸŒ', 'ðŸ˜', 'â¤ï¸', 'â¤', 'ðŸ‘', 'ðŸ˜', 'ðŸ‘½', 'ðŸš€', 'ðŸ“š', 'ðŸ±', 'ðŸŒŒ', 'ðŸ˜©', 'ðŸ¤·\\u200dâ™€ï¸', 'ðŸ˜¬', 'ðŸ¦„', 'ðŸ¤¸\\u200dâ™€ï¸', 'â˜€ï¸', 'ðŸ‘ðŸ¼', 'ðŸ—£', 'ðŸ‘€', 'ðŸ‘ðŸ»', 'ðŸ§‘\\u200dðŸ¤\\u200dðŸ§‘', 'â˜¹ï¸', 'ðŸ¥°', 'ðŸ’–', 'ðŸ¤­', 'ðŸ‘Ž', 'ðŸŽµ', 'ðŸŽ¶', 'ðŸ˜Š', 'ðŸ‘ŒðŸ¼', 'ðŸ¤ŸðŸ¼', 'ðŸ’ªðŸ¼', 'ðŸ”¥', 'ðŸ»', 'ðŸº', 'ðŸ˜‚', 'â™¥ï¸', 'ðŸ¤¡', 'ðŸ’š', 'ðŸ’™', 'ðŸ’œ', 'ðŸ¤”', 'ðŸ¹', 'ðŸ¸', 'ðŸ‘Œ', 'ðŸ ', 'ðŸ™ƒ', 'ðŸš«', 'ðŸ‘‡', 'ðŸ˜†', 'ðŸ˜€', 'ðŸ¤ª', 'ðŸ¤·ðŸ¼\\u200dâ™‚ï¸', 'ðŸ¥º', 'ðŸ“¦', 'ðŸƒ', 'ðŸ’¨', 'ðŸŒ…', 'ðŸ', 'ðŸ‘¬ðŸ»', 'ðŸ¥µ', 'ðŸ’…', 'ðŸ‘¨\\u200dâ¤ï¸\\u200dðŸ‘¨', 'ðŸ‘¨\\u200dâ¤ï¸\\u200dðŸ’‹\\u200dðŸ‘¨', 'ðŸ˜”', 'ðŸ˜‡', 'ðŸ¤«', 'ðŸ™ðŸ»', 'ðŸ¤·\\u200dâ™‚ï¸', 'ðŸ‡§ðŸ‡¿', 'ðŸ˜ž', 'ðŸ“£', 'ðŸ¤¢', 'ðŸ˜ƒ', 'ðŸ˜¡', 'ðŸ’«', 'ðŸ’Ž', 'ðŸ§›\\u200dâ™€ï¸', 'ðŸ¥€', 'ðŸŽ¹', 'â„¢ï¸', 'ðŸ˜º', 'ðŸ¤', 'ðŸ’›', 'ðŸ§¡', 'ðŸ’•', 'âœ‹', 'ðŸ—½', 'ðŸŽƒ', 'ðŸ‡·ðŸ‡º', 'ðŸ¤ ', 'ðŸ’‹', 'ðŸ¤¯', 'ðŸŽ©', 'ðŸ§', 'ðŸ°', 'âœŒï¸', 'ðŸ‘¨', 'ðŸ‘¨ðŸ¼', 'ðŸ¥´', 'ðŸ’”', 'ðŸ‘', 'Â®', 'ðŸ˜Ž', 'ðŸ•', 'ðŸ’©', 'ðŸ˜‰', 'ðŸ§', 'ðŸ‘ðŸ¾', 'ðŸ˜³', 'ðŸ¥³', 'ðŸ‘‡ðŸ½', 'ðŸ”º', 'ðŸ™Œ', 'âŒ', 'â­•', 'ðŸ˜™', 'ðŸ¤¦\\u200dâ™‚ï¸', 'ðŸ¤¦\\u200dâ™‚', 'ðŸ˜‘', 'ðŸ’¥', 'âž¡', 'ðŸ”´', 'ðŸŽ®', 'ðŸ˜', 'ðŸ§µ', 'ðŸ—£ï¸', 'ðŸ’¬', 'ðŸ˜¿', 'ðŸ˜ˆ', 'ðŸ˜', 'ðŸ’Ÿ', 'â–¶ï¸', 'ðŸŒŽ', 'ðŸŒž', 'ðŸ¤®', 'ðŸ’ªðŸ»', 'ðŸ˜’', 'â˜', 'ðŸ¦‰', 'ðŸ™‚', 'ðŸ“–', 'ðŸŽ‰', 'ðŸ¤·ðŸ»\\u200dâ™‚ï¸', 'âœŠðŸ¾', 'ðŸ…°ï¸', 'ðŸ“Œ', 'ðŸ“·', 'ðŸ‘ðŸ¼', 'â”', 'ðŸ‘¶ðŸ»', 'ðŸ‘§ðŸ»', 'ðŸ™ˆ', 'ðŸ˜˜', 'ðŸ‡', 'ðŸ¿', 'âš«', 'ðŸ‘‹', 'ðŸ˜ª', 'ðŸ’‰', 'â˜ ï¸', 'ðŸ‡ºðŸ‡²', 'ðŸ¤¦', 'ðŸ˜¶', 'ðŸ’“', 'â˜º', 'ðŸ³\\u200dðŸŒˆ', 'âš“', 'ðŸ¥‚', 'Â©ï¸', 'âœŠ', '1ï¸âƒ£', '5ï¸âƒ£', 'ðŸ˜', 'ðŸ‡¨ðŸ‡¦', 'ðŸ˜£', 'ðŸš’', 'âœŠðŸ½', 'ðŸ‡ºðŸ‡¦', 'ðŸŽ¥', 'ðŸ¾', 'ðŸš¨', 'â˜ï¸', 'ðŸ¤·ðŸ½\\u200dâ™€ï¸', 'âœï¸', 'â¬‡', 'ðŸŒ»', 'ðŸ˜·', 'ðŸ¦‹', 'ðŸŒ³', 'ðŸ¦¥', 'ðŸŽ¤', 'ðŸ“', 'ðŸ˜„', 'ðŸ˜°', 'ðŸ˜“', 'ðŸ’ðŸ»\\u200dâ™‚ï¸', 'ðŸš©', 'ðŸ¤¦ðŸ¼\\u200dâ™‚ï¸', 'ðŸ’', 'ðŸ‘‘', 'ðŸŽ§', 'ðŸ¤¦ðŸ¾\\u200dâ™‚ï¸', 'ðŸŒ¸', 'ðŸ“', 'ðŸ•', 'ðŸ‘®\\u200dâ™‚ï¸', 'ðŸš”', 'ðŸ‘®\\u200dâ™€ï¸', 'ðŸ¤·ðŸ¼\\u200dâ™€ï¸', 'ðŸ¤¬', 'ðŸ–•', 'ðŸ¤¨', 'ðŸ¤¦ðŸ½\\u200dâ™€ï¸', 'ðŸ•“', 'ðŸ‘¹', 'ðŸ¤©', 'ðŸ‘‰', 'ðŸ‘ˆ', 'ðŸ“¼', 'â˜Ž', 'âœðŸ»', 'ðŸ˜ ', 'ðŸ€', 'ðŸ”—', 'ðŸ¥', 'ðŸ™‹', 'âœŒ', 'ðŸ—‘', 'ðŸ‘®ðŸ»\\u200dâ™‚ï¸', 'ðŸ‘·ðŸ¼\\u200dâ™€ï¸', 'ðŸ˜»', 'ðŸ‡µðŸ‡¸', 'ðŸ‘¾', 'ðŸš', 'â„ï¸', 'ðŸ‡¸ðŸ‡©', 'ðŸ‡¸ðŸ‡²', 'â˜ºï¸', 'ðŸ¦Š', 'ðŸ‘»', 'ðŸ¥±', 'ðŸ¤Ž', 'ðŸ˜š', 'â—', 'ðŸ˜¤', 'ðŸ˜œ', 'ðŸ§‘', 'ðŸŽ¼', 'ðŸ‡¦ðŸ‡«', 'ðŸ‘', 'ðŸ‘ï¸', 'ðŸ™…ðŸ»\\u200dâ™€ï¸', 'ðŸ•ºðŸ½', 'ðŸ˜¨', 'ðŸ˜¥', 'ðŸ“º', 'ðŸŽ¬', 'ðŸ“ž', 'ðŸŒï¸\\u200dâ™‚ï¸', 'â›³', 'ðŸ¨', 'ðŸ”', 'ðŸŸ', 'ðŸ’ª', 'ðŸ‘‡ðŸ»', 'âœŠðŸ¼', 'ðŸ–•ðŸ»', 'ðŸ˜±', 'ðŸ”«', 'ðŸ–•ðŸ¼', 'â™£ï¸', 'ðŸ”–', 'â¬œ', 'ðŸ“•', 'ðŸ™ðŸ¼', 'ðŸ˜¦', 'ðŸ¯', 'ðŸ¦µðŸ¿', 'ðŸ’ƒ', 'ðŸ¦…', 'ðŸŽ‚', 'â¬‡ï¸', 'ðŸ’¸', 'ðŸ’¯', 'âœŠðŸ»', 'ðŸ¦ˆ', 'ðŸ€', 'ðŸ‘ðŸ½', 'â˜®ï¸', 'ðŸ˜«', 'ðŸ™‹ðŸ»\\u200dâ™€ï¸', 'ðŸ‘„', 'ðŸ‚', 'ðŸ§›\\u200dâ™€', 'âš”', 'ðŸ˜Ÿ', 'ðŸ‡·ðŸ‡¸', 'ðŸ‡¬ðŸ‡ª', 'â¤µï¸', 'ðŸ˜‹', 'ðŸ‡§ðŸ‡¬', 'ðŸ‡±ðŸ‡»', 'ðŸ‡·ðŸ‡´', 'ðŸ‡¸ðŸ‡°', 'ðŸ‡¨ðŸ‡¿', 'ðŸ‡µðŸ‡±', 'ðŸ‡±ðŸ‡¹', 'ðŸ‡ªðŸ‡ª', 'ðŸ‡­ðŸ‡º', 'ðŸ‡¸ðŸ‡®', 'ðŸ‡§ðŸ‡¦', 'ðŸ‡²ðŸ‡ª', 'ðŸ‡¦ðŸ‡±', 'ðŸ‡²ðŸ‡°', 'ðŸ‡½ðŸ‡°', 'ðŸ‡­ðŸ‡·', 'ðŸ‡¬ðŸ‡·', 'âœ”ï¸', 'âœŒðŸ»', 'ðŸ', 'ðŸŽ¯', 'ðŸ¤·\\u200dâ™‚', 'ðŸ“¸', 'ðŸ¥', 'âœ', 'ðŸ™ŽðŸ¼\\u200dâ™€ï¸', 'ðŸ™ŽðŸ¾\\u200dâ™€', 'ðŸ™ŒðŸ¿', 'ðŸ‘†', 'ðŸ§\\u200dâ™€ï¸', 'ðŸ¤·ðŸ¼', 'ðŸ’ðŸ¾\\u200dâ™€ï¸', 'âœ‚ï¸', 'â™€ï¸', 'âš°ï¸', 'ðŸ¦´', 'ðŸš¦', 'ðŸš‘', 'â›‘', 'ðŸ¤š', 'ðŸ·', 'ðŸšŒ', 'â€¼ï¸', 'â‰ï¸', 'ðŸ˜®', 'ðŸ‰', 'ðŸ¤¦ðŸ¼\\u200dâ™€ï¸', 'ðŸŒ–', 'ðŸ¦', 'ðŸ„', 'ðŸ§ ', 'â™¥', 'ðŸ˜¯', 'ðŸ‘Š', 'ðŸ“ˆ', 'âš•ï¸', 'ðŸ‡ºðŸ‡³', 'ðŸ¤·', 'ðŸ†“', 'ðŸ“±', 'ðŸ›Œ', 'ðŸŒŠ', 'ðŸˆ', 'ðŸ‘—', 'ðŸ™‰', 'ðŸ´\\u200dâ˜ ï¸', 'ðŸ¤¦ðŸ»', 'âš”ï¸', 'ðŸ‘ ', 'ðŸ˜–', 'ðŸ§»', 'ðŸ¶', 'ðŸ˜›', 'ðŸ¤™', 'ðŸ‘‡ðŸ¼', 'ðŸ¤“', 'âœ¡ï¸', 'ðŸ‡ºðŸ‡¸', 'ðŸ‘ŠðŸ½', 'ðŸ’ªðŸ½', 'ðŸ¿', 'ðŸ™ðŸ¾', 'ðŸ•´', 'ðŸ¤šðŸ»', 'ðŸ¡', 'ðŸ‘©\\u200dðŸ«', 'ðŸ“¢', 'ðŸ¤·ðŸ»\\u200dâ™€ï¸', 'ðŸ‘‰ðŸ¼', 'ðŸ‘ðŸ½', 'ðŸ§\\u200dâ™‚', 'â™‚', 'â™‚ï¸', 'ðŸ‡·ðŸ‡¼', 'ðŸ‡¸ðŸ‡¿', 'ðŸ‡»ðŸ‡³', 'ðŸ‘‹ðŸ»', 'ðŸ¤·ðŸ½\\u200dâ™‚ï¸', 'ðŸŽž', 'ðŸ’¡', 'ðŸ¤¦ðŸ»\\u200dâ™‚ï¸', 'ðŸ¤—', 'ðŸ‡¨ðŸ‡³', 'âšª', 'â˜•', 'ðŸ˜§', 'ðŸ˜²', 'ðŸ§ðŸ»\\u200dâ™‚ï¸', 'ðŸ', 'ðŸ´', 'ðŸ³ï¸', 'ðŸŠ\\u200dâ™€ï¸', 'ðŸš´\\u200dâ™€ï¸', 'ðŸ…', 'ðŸ¥‡', 'ðŸ¥ˆ', 'ðŸ¥‰', 'ðŸ‚', 'ðŸ', 'ðŸ”ï¸', 'ðŸ’»', 'ðŸ´\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f', 'ðŸ´\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f', 'ðŸ´\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f', 'ðŸ‡¬ðŸ‡§', 'ðŸ°', 'ðŸ¤‘', 'ðŸ”½', 'ðŸ–¼ï¸', 'ðŸ™Š', 'âºï¸', 'ðŸ¤·ðŸ¿\\u200dâ™‚ï¸', 'ðŸ¤·ðŸ¾\\u200dâ™‚ï¸', 'ðŸˆ', 'ðŸ†˜', 'ðŸ˜µ', 'ðŸŒ†', 'ðŸš•', 'ðŸ¤²ðŸ½', 'âœï¸', 'ðŸ”Š', 'ðŸ—¿', 'ðŸš', 'ðŸ’ªðŸ¾', 'ðŸ‚', 'ðŸ”ª', 'ðŸ™ðŸ½', 'ðŸ˜¹', 'ðŸ¤¦ðŸ»\\u200dâ™€ï¸', 'ðŸ…¿', 'ðŸ…¾', 'Â®ï¸', 'ðŸ…¾ï¸', 'ðŸ’¤', 'ðŸ…°', 'ðŸ…¿ï¸', 'ðŸ‡©ðŸ‡ª', 'ðŸ’°', 'ðŸ”', 'ðŸ‡³ðŸ‡±', 'ðŸ‘ŒðŸ»', 'ðŸ‡ªðŸ‡¦', 'ðŸ”¯', 'ðŸŽŠ', 'ðŸ¤žðŸ»', 'ðŸ›', 'ðŸ’²', 'ðŸ‡«ðŸ‡·', 'ðŸ‡ªðŸ‡¸', 'ðŸ’‚\\u200dâ™€ï¸', 'ðŸ¸', 'â™€', 'ðŸ—º', 'â°', 'ðŸ‘ˆðŸ¼', 'ðŸ––ðŸ»', 'ðŸŽ', 'ðŸ”µ', 'ðŸ¤¦ðŸ½\\u200dâ™‚ï¸', 'ðŸ¤³', 'ðŸ˜¸', 'ðŸ’’', 'ðŸ›’', 'ðŸ¤', 'ðŸŒ¿', 'ðŸ‡ºðŸ‡¬', 'ðŸ†', 'âœŒðŸ¾', 'â™¿', 'ðŸ‘­', 'ðŸ—³', 'ðŸ‘¥', 'ðŸ”ƒ', 'ðŸ‘', 'ðŸ“…', 'ðŸ›', 'ðŸ’·', 'ðŸ¤', 'ðŸ¤Ÿ', 'ðŸ‘ŽðŸ½', 'â›“ï¸', 'ðŸ€', 'â›ª', 'ðŸ‡¦ðŸ‡º', 'â˜¯', 'ðŸ¦', 'â™¾', 'ðŸ¤ðŸ»', 'ðŸ”·', 'ðŸ•Š', 'ðŸ™…', 'ðŸ’', 'ðŸ¤¦ðŸ¿\\u200dâ™€', 'ðŸ™', 'ðŸ¤–', 'ðŸ‡¨ðŸ‡®', 'ðŸ’…ðŸ»', 'ðŸ‡¸ðŸ‡´', 'ðŸ‡ºðŸ‡¿', 'ðŸ¤·ðŸ¾\\u200dâ™€ï¸', 'ðŸŽ²', 'ðŸ‘­ðŸ¾', 'ðŸ’…ðŸ¼', 'ðŸ ', 'ðŸ›¸', 'ðŸ¤˜', 'ðŸŒš', 'ðŸ”„', 'âœŒðŸ¼', 'ðŸ“', 'âš¡', 'ðŸ’…ðŸ¾', 'ðŸŒ', 'ðŸ§', 'ðŸ¤·ðŸ»', 'ðŸ¦†', 'ðŸ¤', 'ðŸ”', 'ðŸ—‘ï¸', 'ðŸ§š\\u200dâ™‚ï¸', 'ðŸ“˜', 'ðŸ’ž', 'ðŸ‡¬ðŸ‡­', 'âš–', 'ðŸŽŸ', 'âœï¸', 'âš½', 'ðŸƒ', 'â˜”', 'ðŸ‘¼', 'ðŸ†’', 'ðŸ“', 'â¬†ï¸', 'âœ‰', 'ðŸ¦¾', 'ðŸ…±', 'ðŸ‡«ðŸ‡¯', 'ðŸ‡²ðŸ‡¹', 'ðŸ¥–', 'ðŸŽ', 'ðŸ¥—', 'ðŸ¨', 'âœŠðŸ¿', 'ðŸŒ', 'ðŸŠ\\u200dâ™‚ï¸', 'ðŸ‘™', 'âž–', 'âš ï¸', 'ðŸ¥ƒ', 'ðŸ¥©', 'ðŸ‘ŒðŸ½', 'ðŸ¤žðŸ½', 'â±', 'ðŸ‘©\\u200dðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘¦', 'ðŸ¦§', 'ðŸ’†ðŸ»\\u200dâ™€', 'ðŸ‡ªðŸ‡º', 'â˜‘ï¸', 'ðŸŒ´', 'ðŸ“½', 'â£ï¸', 'ðŸŽ¸', 'ðŸ¤ž', 'ðŸ§’', 'ðŸ¤¸\\u200dâ™‚ï¸', 'ðŸ‡²ðŸ‡½', 'ðŸ‡»ðŸ‡¦', 'ðŸ˜', 'ðŸŽžï¸', 'ðŸ¥š', 'ðŸ’', 'ðŸ’¢', 'Â©', 'ðŸ‘¬', 'ðŸµ', 'ðŸ“†', 'ðŸ‡²ðŸ‡»', 'ðŸ‡¸ðŸ‡¨', 'ðŸ‡¹ðŸ‡·', 'ðŸ“°', 'ðŸ”®', 'ðŸŒ™', 'ðŸ¤²ðŸ¼', 'ðŸŽ“', 'ðŸŽ­', 'ðŸ’…ðŸ½', 'ðŸ“¹', 'ðŸ“›', 'ðŸ’£', 'ðŸ•¸ï¸', 'ðŸ§¨', 'ðŸŸ¢', 'ðŸ–¥ï¸', 'ðŸž', 'ðŸ¥¯', 'ðŸ‘¯\\u200dâ™‚ï¸', 'ðŸ”¨', 'ðŸ¥ª', 'ðŸ’Š', 'ðŸ™‹ðŸ½\\u200dâ™€', 'ðŸŒ', 'ðŸž', 'ðŸ•µ', 'ðŸ“œ', 'ðŸ¤¦ðŸ¼\\u200dâ™€', 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘¨\\u200dðŸ‘§\\u200dðŸ‘§', 'ðŸ‡µðŸ‡­', 'ðŸ¤°ðŸ¼', 'ðŸ”', 'â“', 'ðŸ‡¨ðŸ‡´', 'ðŸŽ™ï¸', 'ðŸ§ðŸ½\\u200dâ™€ï¸', 'âœ”', 'ðŸ¤¦\\u200dâ™€', 'ðŸ‡¯ðŸ‡µ', 'ðŸ•–', 'ðŸ‡®ðŸ‡¹', 'ðŸ¤˜ðŸ½', 'ðŸ½', 'ðŸ™ŒðŸ»', 'ðŸš‚', 'ðŸš—', 'ðŸ¦–', 'ðŸ‘‰ðŸ»', 'ðŸ¤²', 'ðŸ“²', 'ðŸŽ¾', 'ðŸ·', 'ðŸŽ', 'ðŸ”ž', 'ðŸ•¸', 'â˜¹', 'ðŸ‹ðŸ¾\\u200dâ™€ï¸', 'ðŸ¤™ðŸ¾', 'ðŸ†™', 'ðŸŒ‹', 'ðŸ‘¿', 'ðŸ¤™ðŸ¼', 'ðŸ†', 'ðŸ•¤', 'ðŸŽ«', 'ðŸ', 'ðŸ‡±ðŸ‡°', 'ðŸ§šðŸ¾\\u200dâ™‚ï¸', 'ðŸ¡', 'ðŸ—³ï¸', 'ðŸ–¥', 'ðŸ†•', 'ðŸ“»', 'ðŸŽšï¸', 'ðŸ™ðŸ¿', 'ðŸ”¹', 'ðŸ‡§ðŸ‡´', 'â˜®', 'ðŸ•¹ï¸', 'ðŸ’­', 'ðŸ¦•', 'ðŸš®', 'ðŸ’¿', 'ðŸ§˜ðŸ½\\u200dâ™‚ï¸', 'ðŸŽŸï¸', 'ðŸ‹', 'ðŸ­', 'ðŸ§€', 'ðŸ¥¤', 'ðŸŽ™', 'ðŸ¤¦ðŸ¼', 'âœŒðŸ½', 'â™Ÿ', 'ðŸ’¦', 'ðŸ‘…', 'ðŸ‡®ðŸ‡ª', 'ðŸ¤•', 'ðŸŒ¼', 'ðŸ”', 'ðŸ‡¹ðŸ‡¹', 'ðŸ‡²ðŸ‡²', 'ðŸ‡¹ðŸ‡³', 'ðŸ‘©\\u200dðŸ¦³', 'ðŸ¢', 'ðŸ‘©', 'ðŸ‘©ðŸ¾', 'ðŸ›‘', 'âš ', 'â˜Žï¸', 'âš–ï¸', 'ðŸ¦˜', 'ðŸ‡®ðŸ‡±', 'ðŸ•µðŸ½\\u200dâ™‚ï¸', 'ðŸ’˜', 'ðŸ§\\u200dâ™€', 'ðŸ™†\\u200dâ™€ï¸', 'ðŸ’Œ', 'ðŸ˜´', 'ðŸ¤·ðŸ¾', 'ðŸ‡¦ðŸ‡²', 'âœ–', 'ðŸ¤¦ðŸ¾\\u200dâ™€', 'ðŸ‘­ðŸ»', 'ðŸ‘¬ðŸ½', 'ðŸ¦¦', 'ðŸ‘‡ðŸ¿', 'ðŸ™ŒðŸ¾', 'ðŸ¤¦ðŸ»\\u200dâ™€', 'ðŸ—“', 'ðŸ‘°', 'ðŸ–•ðŸ¾', 'ðŸ‡¨ðŸ‡º', 'ðŸ™…ðŸ¿\\u200dâ™‚ï¸', 'ðŸ¤’', 'ðŸ‡¸ðŸ‡±', 'ðŸ‡µðŸ‡¹', 'â³', 'ðŸ–ðŸ»', 'ðŸ“Ž', 'ðŸ§ðŸ½', 'ðŸ•º', 'ðŸ“§', 'ðŸŽ·', 'ðŸ¦œ', 'ðŸ™†ðŸ¾\\u200dâ™‚ï¸', 'âš¾', 'â˜ðŸ¾', 'ðŸ‘', 'ðŸ‘‰ðŸ¾', 'ðŸ‘ˆðŸ¾', 'ðŸ˜—', 'ðŸ‘©ðŸ¾\\u200dðŸ’»', 'ðŸ¤¸ðŸ»\\u200dâ™€ï¸', 'â˜ðŸ»', 'â˜€', 'ðŸŽ¨', 'ðŸ’„', 'ðŸ¥Š', 'ðŸŒ§', 'ðŸ¥Ÿ', 'ðŸ±', 'ðŸœ', 'ðŸ¾', 'ðŸ§Ÿ', 'ðŸ’µ', 'ðŸ¤¥', 'ðŸ–¼', 'ðŸ›¤ï¸', 'ðŸ©¸', 'â¬…ï¸', 'ðŸ´', 'ðŸš­', 'ðŸ’\\u200dâ™‚ï¸', 'ðŸ¤¦ðŸ¿\\u200dâ™‚ï¸', 'âŽ', 'ðŸš¬', 'ðŸ’ðŸ¼\\u200dâ™‚ï¸', 'ðŸ’ðŸ½\\u200dâ™‚ï¸', 'ðŸ’ðŸ¾\\u200dâ™‚ï¸', 'ðŸ’ðŸ¿\\u200dâ™‚ï¸', 'ðŸ¥¶', 'ðŸ•\\u200dðŸ¦º', 'â›±ï¸', 'ðŸ—¡', 'ðŸ•', 'ðŸŒ­', 'ðŸ™‹\\u200dâ™‚ï¸', 'ðŸ‡¨ðŸ‡­', 'ðŸ€„', 'ðŸ–ï¸', 'â˜¯ï¸', 'ðŸ¤·ðŸ»\\u200dâ™‚', 'ðŸŒ¤', 'ðŸŒ¦', 'ðŸ°', 'ðŸ’', 'ðŸƒðŸ½\\u200dâ™€ï¸', 'â¬›', 'ðŸ‡¸ðŸ‡³', 'ðŸ‡®ðŸ‡´', 'ðŸ›¶', 'ðŸ§˜ðŸ½\\u200dâ™‚', 'ðŸ¦', 'ðŸƒ', 'ðŸ', 'ðŸ†—', 'ðŸŒ«ï¸', 'ðŸ–ï¸', 'ðŸ¥Ž', 'ðŸ‘‡ðŸ¾', 'ðŸ§\\u200dâ™€ï¸', 'â¬†', 'ðŸ¥', 'â›²', 'ðŸ¥•', 'ðŸ¤·\\u200dâ™€', 'â˜„ï¸', 'ðŸ–•ðŸ½', 'ðŸª‘', 'ðŸ›', 'ðŸ‹ðŸ»\\u200dâ™€ï¸', 'â˜ ', 'ðŸŒï¸', 'ðŸŒ¡ï¸', 'ðŸ—¡ï¸', 'âš™ï¸', 'ðŸ‡¸ðŸ‡¹', 'ðŸ‡¹ðŸ‡¯', 'ðŸ”±', 'ðŸ‘£', 'ðŸ‡©ðŸ‡°', 'ðŸ–‹', 'ðŸ“¨', 'ðŸ‘', 'ðŸŽ€', 'â©', 'ðŸ™', 'ðŸ™‹\\u200dâ™€', 'â™ ï¸', 'ðŸ§™\\u200dâ™€ï¸', 'ðŸŒ¶ï¸', 'ðŸ•¶ï¸', 'â›”', 'â˜¸ï¸', 'âº', 'ðŸ¤¦ðŸ½', 'ðŸ™‹ðŸ¾\\u200dâ™€ï¸', 'ðŸ—ï¸', 'ðŸƒðŸ»', 'ðŸŒ·', 'ðŸ¤·ðŸ¿\\u200dâ™€ï¸', 'ðŸ', 'ðŸ“³', 'ðŸ‘¨\\u200dâš–ï¸', 'â›º', 'ðŸ•Œ', 'ðŸ‘ðŸ¾', 'ðŸ§”', 'ðŸ‘ŒðŸ¿', 'ðŸœ', 'ðŸ‡«ðŸ‡®', 'ðŸ–', 'ðŸ©±', 'ðŸ™…\\u200dâ™€ï¸', 'ðŸ§šðŸ¼', 'ðŸ¦ƒ', 'ðŸ¤˜ðŸ¼', 'ðŸ‡§ðŸ‡·', 'ðŸ‡¸ðŸ‡­', 'ðŸ‡¹ðŸ‡¼', 'ðŸŽ»', 'âœ‰ï¸', 'ðŸ’§', 'ðŸŒ¶', 'ðŸ', 'ðŸ§¶', 'ðŸ’ðŸ»\\u200dâ™€ï¸', 'ðŸ¥', 'ðŸ‘¸ðŸ»', 'ðŸŒ•', 'â™‹', 'ðŸ¨', 'ðŸ·', 'ðŸŒª', 'ðŸ§š', 'ðŸ¤¦ðŸ»\\u200dâ™‚', 'ðŸ‘¨\\u200dâš•ï¸', 'ðŸš¢', 'ðŸ’¼', 'ðŸ', 'ðŸ¤¸ðŸ½\\u200dâ™€ï¸', 'ðŸ‘©\\u200dðŸŽ“', 'ðŸ‘¨\\u200dðŸŽ“', 'ðŸ©', 'ðŸ“Š', 'ðŸ™†ðŸ»\\u200dâ™€ï¸', 'ðŸ‘¼ðŸ»', 'ðŸŒ®', 'ðŸ‡³ðŸ‡´', 'ðŸ‡¹ðŸ‡­', 'ðŸ‘‰ðŸ½', 'ðŸšª', 'ðŸ‹\\u200dâ™‚ï¸', 'ðŸ’ƒðŸ¼', 'ðŸ¦ ', 'ðŸ–Œï¸', 'ðŸ´\\u200dâ˜ ', 'ðŸ›©', 'ðŸ•Šï¸', 'ðŸ§ŽðŸ»\\u200dâ™‚', 'ðŸ‘ž', 'ðŸ‘©ðŸ»', 'ðŸ§ðŸ½\\u200dâ™‚ï¸', 'ðŸ¦‚', 'ðŸ‡·ðŸ‡ª', 'ðŸŒ±', 'ðŸ¤¦ðŸ½\\u200dâ™‚', 'ðŸ•‹', 'ðŸ“½ï¸', 'ðŸ‡±ðŸ‡º', 'ðŸ‘ŽðŸ»', 'ðŸ§ðŸ»', 'ðŸŒ', 'ðŸŸ ', 'ðŸ’\\u200dâ™€ï¸', 'ðŸ‡§ðŸ‡²', 'ðŸ¥¥', 'ðŸ™…\\u200dâ™‚ï¸', 'â˜ªï¸', 'ðŸ¤œ', 'ðŸ¤›', 'â˜¦ï¸', '1âƒ£', 'ðŸ“‰', 'ðŸ‡²ðŸ‡¾', 'ðŸ˜¼', 'ðŸ‘‹ðŸ¼', 'ðŸ™‹ðŸ»\\u200dâ™‚ï¸', 'ðŸ›¹', 'ðŸ‡¨ðŸ‡¬', 'ðŸ‡²ðŸ‡¨', 'ðŸ‡¸ðŸ‡¾', 'ðŸ‘†ðŸ¼', 'ðŸ‘º', 'ðŸ”™', 'ðŸ‘¨ðŸ½\\u200dðŸ¦±', 'ðŸ‘¨ðŸ¾\\u200dðŸ¦±', 'ðŸ‡²ðŸ‡©', 'ðŸ‡¸ðŸ‡¬', 'ðŸ„', 'ðŸ”Ž', 'ðŸ’ðŸ»', 'ðŸƒðŸ¼\\u200dâ™€ï¸', 'ðŸ¦€', 'ðŸŒï¸\\u200dâ™€ï¸', 'ðŸ¦¶', 'ðŸ²', 'ðŸ¤°ðŸ½', 'ðŸ™‡\\u200dâ™‚ï¸', 'ðŸ™‡\\u200dâ™€ï¸', 'ðŸ›ï¸', 'ðŸ’‘', 'ðŸ‘©\\u200dâ¤ï¸\\u200dðŸ‘©', 'ðŸ§˜\\u200dâ™‚ï¸', 'â›°ï¸', 'ðŸ—', 'ðŸ›«', 'ðŸŒƒ', 'ðŸ›¬', 'ðŸ˜¾', 'ðŸŽ¡', 'ðŸš·', 'ðŸ˜½', 'ðŸ‘¨ðŸ½', 'ðŸ”…', 'â›½', 'ðŸ’', 'ðŸŠ', 'ðŸ§˜', 'ðŸ’º', 'ðŸ¼', 'ðŸ‡¦ðŸ‡ª', 'âŒš', 'ðŸ•ºðŸ»', 'ðŸ’…ðŸ¿', 'ðŸ‡§ðŸ‡©', 'ðŸ‘©\\u200dâ¤ï¸\\u200dðŸ’‹\\u200dðŸ‘©', 'ðŸ¤¦ðŸ¾\\u200dâ™‚', 'ðŸ•¯', 'ðŸ§¦', 'ðŸ‰', 'ðŸ‘¦', 'ðŸ‘§', 'âœˆï¸', 'ðŸ§¿', 'ðŸ»', 'ðŸŒ¥', 'ðŸ‡°ðŸ‡ª', 'ðŸ‡°ðŸ‡¬', 'ðŸ‡»ðŸ‡¨', 'ðŸ‡­ðŸ‡²', 'ðŸ“´', 'â™Š', 'ðŸŽ¦', 'ðŸ––', 'ðŸ¤·ðŸ»\\u200dâ™€', 'ðŸ•°', 'â™ ', 'ðŸ™€', 'ðŸ™†\\u200dâ™‚ï¸', 'ðŸ‡°ðŸ‡·', 'ðŸ“®', 'ðŸ‘‹ðŸ¿', 'ðŸ“', 'ðŸŽˆ', 'ðŸ‘¨ðŸ»\\u200dðŸ¦°', 'ðŸ§Ž\\u200dâ™€ï¸', 'ðŸ§›\\u200dâ™‚ï¸', 'ðŸ§±', 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘¦', 'âœ–ï¸', 'ðŸ¤™ðŸ»', 'ðŸ•¶', 'ðŸº', 'â–¶', 'ðŸ‡¶ðŸ‡¦', 'ðŸ”§', 'ðŸ¤¦ðŸ¾\\u200dâ™€ï¸', 'ðŸ‘©ðŸ½\\u200dðŸ¦½', 'ðŸ¤·ðŸ½', 'ðŸƒðŸ»\\u200dâ™€', 'ðŸ”•', 'ðŸ‡¦ðŸ‡¬', 'ðŸ‡§ðŸ‡³', 'ðŸ‡¸ðŸ‡¦', 'ðŸ¤žðŸ¼', 'ðŸ‘Ÿ', 'ðŸ’ƒðŸ»', 'ðŸ‡³ðŸ‡¬', 'ðŸ‡µðŸ‡°', 'ðŸ‡°ðŸ‡³', 'ðŸ•µï¸\\u200dâ™‚ï¸', 'ðŸ¦', 'ðŸ™ŒðŸ¼', 'ðŸ§ðŸ¼\\u200dâ™‚ï¸', 'ðŸ§˜\\u200dâ™€ï¸', 'ðŸŒ½', 'ðŸƒ\\u200dâ™€ï¸', 'ðŸ¦¸', 'ðŸ¦¹', 'ðŸ«', 'ðŸ‡­ðŸ‡°', 'ðŸ™…ðŸ¼\\u200dâ™€ï¸', 'â˜', 'ðŸ‡®ðŸ‡¶', 'ðŸŒº', 'ðŸ‡', 'â™¨ï¸', 'ðŸ‘ˆðŸ»', 'ðŸ‡®ðŸ‡©', '5âƒ£', 'ðŸ‘¤', 'â™‰', 'ðŸ‘•', 'ðŸ‡³ðŸ‡ª', 'ðŸ‡§ðŸ‡±', 'ðŸ§¢', 'ðŸ‘ðŸ¿', 'â˜¢', 'â˜¢ï¸', 'ðŸ’ƒðŸ¾', 'ðŸ¦‡', 'ðŸ§›ðŸ»\\u200dâ™€ï¸', 'ðŸ‘©ðŸ¿', 'ðŸ‘´', 'ðŸ§¸', 'ðŸ‘¸', 'ðŸŒ¬', 'ðŸ“™', 'ðŸ§™\\u200dâ™‚ï¸', 'ðŸŸ¥', 'ðŸŸ©', 'ðŸ©²', 'ðŸ¤™ðŸ½', 'ðŸ‘©ðŸ½', 'ðŸŒ²', 'ðŸ‡³ðŸ‡¦', 'ðŸ‡±ðŸ‡®', 'ðŸ‡´ðŸ‡²', 'ðŸ‡±ðŸ‡¨', 'ðŸ§\\u200dâ™‚ï¸', 'âšœï¸', 'â•', 'ðŸ‡µðŸ‡·', 'ðŸ•‰ï¸', 'ðŸ‘¨\\u200dðŸš’', 'ðŸ', 'ðŸŒ ', 'ðŸ‡ºðŸ‡¾', 'ðŸ‘ðŸ¿', 'ðŸ‡®ðŸ‡³', 'ðŸ‘©\\u200dâœˆï¸', 'ðŸ–Š', 'ðŸ‡¨ðŸ‡±', 'ðŸš‰', 'ðŸ’´', 'ðŸ§ŽðŸ¾', 'ðŸŠ', 'ðŸ“©', 'ðŸ‘‚', 'âš’ï¸', 'ðŸ‘¨ðŸ»\\u200dðŸ’»', 'ðŸ‘©ðŸ¼\\u200dðŸ’¼', 'ðŸ¥‹', 'ðŸ‘‰ðŸ¿', 'ðŸ‘ª', 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘§', 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘¦\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘§', 'ðŸ‘©\\u200dðŸ‘©\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘©\\u200dðŸ‘§', 'ðŸ‘©\\u200dðŸ‘©\\u200dðŸ‘¦\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘§', 'ðŸ‘¨\\u200dðŸ‘¨\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘¨\\u200dðŸ‘§', 'ðŸ‘¨\\u200dðŸ‘¨\\u200dðŸ‘§\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘¨\\u200dðŸ‘¦\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘§', 'ðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘¦\\u200dðŸ‘¦', 'ðŸ‘©\\u200dðŸ‘§\\u200dðŸ‘§', 'ðŸ‘¨\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘§', 'ðŸ‘¨\\u200dðŸ‘§\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘¦\\u200dðŸ‘¦', 'ðŸ‘¨\\u200dðŸ‘§\\u200dðŸ‘§', 'ðŸ‘©ðŸ¿\\u200dðŸ¤\\u200dðŸ‘©ðŸ»', 'ðŸ‘¨ðŸ¾\\u200dðŸ”§', 'ðŸ‡²ðŸ‡¿', 'ðŸ‡³ðŸ‡µ', 'ðŸ‡µðŸ‡²', 'ðŸ™…ðŸ¾\\u200dâ™‚ï¸', 'ðŸƒ', 'ðŸ•³ï¸']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emoji_df = pd.read_csv('emoji.csv')\n",
    "emoji_list = emoji_df['emoji'].to_list()\n",
    "print(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2b2trans', '2lgbtqias', '2nite', 'aam', 'abbasi', 'abstrak', 'ade', 'adha', 'ael', 'ako', 'aku', 'aly', 'anarcha', 'anda', 'andalus', 'apa', 'as1anslgbt2', 'askip', 'assh0le', 'atonia', 'autsti', 'awwww', 'ay', 'bagus', 'bahan', 'bajan', 'basta', 'bearagon', 'behan', 'bencana', 'benda', 'berat', 'biden', 'bonang', 'bradbury', 'brancher', 'brexit', 'briton', 'broduil', 'cardia', 'carlin', 'cee', 'cfp', 'chakra', 'charle', 'chauvin', 'chay', 'chaya', 'chica', 'chucky', 'clawdeen', 'cleo', 'corruptor', 'covid', 'covid-19', 'cpa', 'crockett', 'cupcakke', 'daphshe', 'dari', 'dragulaura', 'elysia', 'etcnon', 'faham', 'fair1', 'familia', 'fascistspro', 'feminin', 'fication', 'fil', 'firefang', 'fishman', 'furrie', 'gaymo', 'gc', 'georgiafreedom', 'gette', 'goldwater', 'goli', 'gop', 'grat', 'grian', 'grt', 'gta', 'gue', 'hahaha', 'hairdress', 'hak', 'hamadani', 'hao', 'hoi', 'hud', 'ig', 'inb4', 'ini', 'iquity', 'isak', 'jure', 'jynx', 'kadhi', 'kaidan', 'kal', 'kalo', 'karlsberg', 'kashi', 'kenasia', 'kenisha', 'khutbah', 'koda', 'kulah', 'kulit', 'kurd', 'l3sbians', 'latifi', 'lesbianaclaw', 'lesbianlie', 'lgbtabc123xyz', 'lgbtiqa', 'lgbtliberty', 'lgbtq', 'lgbtq2', 'lgbtqia2', 'lgbtÃ·', 'lik', 'lipsky', 'localizer', 'looney', 'loverjupiter', 'lvt', 'ly', 'maga', 'mang', 'mannan', 'mansouri', 'masa', 'mau', 'mec', 'meio', 'merc', 'misandry', 'mitsuba', 'monogramme', 'mou', 'naam', 'nakong', 'nea', 'nep', 'nivea', 'nye', 'nyo', 'oda', 'oldsthese', 'ortho', 'pansim', 'pantalon', 'passionifyco', 'peasy', 'peele', 'pepperidge', 'polska', 'pongorma', 'positivi', 'potong', 'ppls', 'proship', 'proshpped0', 'psa', 'pst', 'purga', 'quebical', 'queerbaiting', 'quei', 'quigley', 'ransel', 'rated18', 'reis', 'ret', 'rogan', 'rostow', 'rp', 'rrow', 'sabina', 'sadr', 'sama', 'sandinista', 'saya', 'seung', 'shaykh', 'sisi', 'skÅ‚odowska', 'sok', 'sooo', 'soooo', 'spiegel', 'spina', 'stylo', 'su1c1de', 'tec', 'thanksgive', 'themis', 'titanbi', 'toon', 'utili', 'vag', 'vinny', 'wha', 'wickham', 'wtttffffff', 'ww2', 'xtian', 'yagi', 'zacatecas', 'zahra', 'æ–°äººvtuber']\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('vocab.csv')\n",
    "vocab_list = vocab_df['word'].to_list()\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1384 tokens\n"
     ]
    }
   ],
   "source": [
    "new_tokens = emoji_list + vocab_list\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    " # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-2b0b4ce672e3e08f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac45d297ec48a9bc697d0fb91cf4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-8d5d2109338c8aa8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 59333\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 19778\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 19778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize_function, batched=True, batch_size=None)\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['final', 'ly', 'my', 'first', 'politician', 'blocked', 'me', 'i', 'am', 'feeling', 'of', 'myself', 'r', 'ig', 'h', '##t', 'now', 'ðŸ˜†']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('finally my first politician blocked me i am feeling of myself right now ðŸ˜†'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'you do not know how homosexuality is related to teaching propaganda to and showing them drag queen you really can not make that connection', 'label': 2, 'input_ids': [101, 2017, 2079, 2025, 2113, 2129, 15949, 2003, 3141, 2000, 4252, 17678, 8490, 31705, 2000, 1998, 4760, 2068, 8011, 3035, 2017, 2613, 31821, 2064, 2025, 2191, 2008, 4434, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded['train'][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(31906, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 3\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "         .to(device))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim='adamw_torch'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d7301225d0438a844afc2562be870c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 4.00 GiB total capacity; 2.73 GiB already allocated; 0 bytes free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\CZ4045---NLP\\BERT_model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                   compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                   args\u001b[39m=\u001b[39mtraining_args, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                   train_dataset\u001b[39m=\u001b[39mdataset_encoded[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                   eval_dataset\u001b[39m=\u001b[39mdataset_encoded[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                   tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2498\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2499\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2502\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2531\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2532\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2533\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1560\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1560\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1561\u001b[0m     input_ids,\n\u001b[0;32m   1562\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1563\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1564\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1565\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1566\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1567\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1568\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1569\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1570\u001b[0m )\n\u001b[0;32m   1572\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\activations.py:56\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 4.00 GiB total capacity; 2.73 GiB already allocated; 0 bytes free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  args=training_args, \n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6fe2028bd84526aefa5ddb41cf25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06547744572162628,\n",
       " 'eval_accuracy': 0.9843710612553567,\n",
       " 'eval_runtime': 121.6135,\n",
       " 'eval_samples_per_second': 97.859,\n",
       " 'eval_steps_per_second': 6.118,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f4dac35224d53844635d1d3bbcc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0853798389434814,\n",
       " 'eval_accuracy': 0.7842198134610537,\n",
       " 'eval_runtime': 43.2866,\n",
       " 'eval_samples_per_second': 91.645,\n",
       " 'eval_steps_per_second': 5.729,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b49ebe8e4241189f55349e5b02f6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output=trainer.predict(dataset_encoded[\"test\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 556,   83,   92],\n",
       "       [  69, 1171,  246],\n",
       "       [  87,  279, 1384]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(dataset_encoded[\"test\"][\"label\"],predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased-finetuned'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "classifier = pipeline('text-classification', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"monkey pox is a hoax all the community hah ðŸ¤¡ ðŸ³ï¸â€ðŸŒˆ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {\n",
    "    'LABEL_0' : 'NEUTRAL',\n",
    "    'LABEL_1' : 'NEGATIVE',\n",
    "    'LABEL_2' : 'POSITIVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: monkey pox is a hoax all the community hah ðŸ¤¡ ðŸ³ï¸â€ðŸŒˆ\n",
      "\n",
      "This sentence is classified with a NEGATIVE sentiment\n"
     ]
    }
   ],
   "source": [
    "c = classifier(sentence)\n",
    "print(f'\\nSentence: {sentence}')\n",
    "print(f\"\\nThis sentence is classified with a {classifier_dict[c[0]['label']]} sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
