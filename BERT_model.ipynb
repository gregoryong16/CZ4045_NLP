{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1381fbead958f0e2\n",
      "Reusing dataset csv (C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014cebec1eb64ac0b8544a14793f219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = './data/'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': base_url+'bert_train.csv','validation': base_url+'bert_val.csv','test': base_url+'bert_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['™', '✅', '💗', '😢', '🏳️\\u200d🌈', '😌', '🤦\\u200d♀️', '🏳', '💀', '🌈', '👏🏻', '✨', '🌟', '🌹', '😕', '⭐', '😅', '🙏', '🐒', '🧐', '😭', '🙄', '🖤', '🤧', '🤣', '🗃️', '🗓️', '➡️', '🔻', '🤤', '🌐', '😁', '❤️', '❤', '👏', '😍', '👽', '🚀', '📚', '🐱', '🌌', '😩', '🤷\\u200d♀️', '😬', '🦄', '🤸\\u200d♀️', '☀️', '👏🏼', '🗣', '👀', '👍🏻', '🧑\\u200d🤝\\u200d🧑', '☹️', '🥰', '💖', '🤭', '👎', '🎵', '🎶', '😊', '👌🏼', '🤟🏼', '💪🏼', '🔥', '🐻', '🐺', '😂', '♥️', '🤡', '💚', '💙', '💜', '🤔', '🍹', '🍸', '👌', '🏠', '🙃', '🚫', '👇', '😆', '😀', '🤪', '🤷🏼\\u200d♂️', '🥺', '📦', '🍃', '💨', '🌅', '🏝', '👬🏻', '🥵', '💅', '👨\\u200d❤️\\u200d👨', '👨\\u200d❤️\\u200d💋\\u200d👨', '😔', '😇', '🤫', '🙏🏻', '🤷\\u200d♂️', '🇧🇿', '😞', '📣', '🤢', '😃', '😡', '💫', '💎', '🧛\\u200d♀️', '🥀', '🎹', '™️', '😺', '🤍', '💛', '🧡', '💕', '✋', '🗽', '🎃', '🇷🇺', '🤠', '💋', '🤯', '🎩', '🧁', '🍰', '✌️', '👨', '👨🏼', '🥴', '💔', '👍', '®', '😎', '🐕', '💩', '😉', '🐧', '👍🏾', '😳', '🥳', '👇🏽', '🔺', '🙌', '❌', '⭕', '😙', '🤦\\u200d♂️', '🤦\\u200d♂', '😑', '💥', '➡', '🔴', '🎮', '😐', '🧵', '🗣️', '💬', '😿', '😈', '😏', '💟', '▶️', '🌎', '🌞', '🤮', '💪🏻', '😒', '☝', '🦉', '🙂', '📖', '🎉', '🤷🏻\\u200d♂️', '✊🏾', '🅰️', '📌', '📷', '👍🏼', '❔', '👶🏻', '👧🏻', '🙈', '😘', '🐇', '🐿', '⚫', '👋', '😪', '💉', '☠️', '🇺🇲', '🤦', '😶', '💓', '☺', '🏳\\u200d🌈', '⚓', '🥂', '©️', '✊', '1️⃣', '5️⃣', '😝', '🇨🇦', '😣', '🚒', '✊🏽', '🇺🇦', '🎥', '🍾', '🚨', '☝️', '🤷🏽\\u200d♀️', '✍️', '⬇', '🌻', '😷', '🦋', '🌳', '🦥', '🎤', '📝', '😄', '😰', '😓', '💁🏻\\u200d♂️', '🚩', '🤦🏼\\u200d♂️', '💝', '👑', '🎧', '🤦🏾\\u200d♂️', '🌸', '📍', '🕐', '👮\\u200d♂️', '🚔', '👮\\u200d♀️', '🤷🏼\\u200d♀️', '🤬', '🖕', '🤨', '🤦🏽\\u200d♀️', '🕓', '👹', '🤩', '👉', '👈', '📼', '☎', '✍🏻', '😠', '🐀', '🔗', '🏥', '🙋', '✌', '🗑', '👮🏻\\u200d♂️', '👷🏼\\u200d♀️', '😻', '🇵🇸', '👾', '🍚', '❄️', '🇸🇩', '🇸🇲', '☺️', '🦊', '👻', '🥱', '🤎', '😚', '❗', '😤', '😜', '🧑', '🎼', '🇦🇫', '👁', '👁️', '🙅🏻\\u200d♀️', '🕺🏽', '😨', '😥', '📺', '🎬', '📞', '🏌️\\u200d♂️', '⛳', '🍨', '🍔', '🍟', '💪', '👇🏻', '✊🏼', '🖕🏻', '😱', '🔫', '🖕🏼', '♣️', '🔖', '⬜', '📕', '🙏🏼', '😦', '🐯', '🦵🏿', '💃', '🦅', '🎂', '⬇️', '💸', '💯', '✊🏻', '🦈', '🏀', '👍🏽', '☮️', '😫', '🙋🏻\\u200d♀️', '👄', '🍂', '🧛\\u200d♀', '⚔', '😟', '🇷🇸', '🇬🇪', '⤵️', '😋', '🇧🇬', '🇱🇻', '🇷🇴', '🇸🇰', '🇨🇿', '🇵🇱', '🇱🇹', '🇪🇪', '🇭🇺', '🇸🇮', '🇧🇦', '🇲🇪', '🇦🇱', '🇲🇰', '🇽🇰', '🇭🇷', '🇬🇷', '✔️', '✌🏻', '🐝', '🎯', '🤷\\u200d♂', '📸', '🐥', '✍', '🙎🏼\\u200d♀️', '🙎🏾\\u200d♀', '🙌🏿', '👆', '🧍\\u200d♀️', '🤷🏼', '💁🏾\\u200d♀️', '✂️', '♀️', '⚰️', '🦴', '🚦', '🚑', '⛑', '🤚', '🏷', '🚌', '‼️', '⁉️', '😮', '🏉', '🤦🏼\\u200d♀️', '🌖', '🦁', '🍄', '🧠', '♥', '😯', '👊', '📈', '⚕️', '🇺🇳', '🤷', '🆓', '📱', '🛌', '🌊', '🐈', '👗', '🙉', '🏴\\u200d☠️', '🤦🏻', '⚔️', '👠', '😖', '🧻', '🐶', '😛', '🤙', '👇🏼', '🤓', '✡️', '🇺🇸', '👊🏽', '💪🏽', '🍿', '🙏🏾', '🕴', '🤚🏻', '🏡', '👩\\u200d🏫', '📢', '🤷🏻\\u200d♀️', '👉🏼', '👏🏽', '🧍\\u200d♂', '♂', '♂️', '🇷🇼', '🇸🇿', '🇻🇳', '👋🏻', '🤷🏽\\u200d♂️', '🎞', '💡', '🤦🏻\\u200d♂️', '🤗', '🇨🇳', '⚪', '☕', '😧', '😲', '🧍🏻\\u200d♂️', '🏁', '🏴', '🏳️', '🏊\\u200d♀️', '🚴\\u200d♀️', '🏅', '🥇', '🥈', '🥉', '🐂', '🐍', '🏔️', '💻', '🏴\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f', '🇬🇧', '🐰', '🤑', '🔽', '🖼️', '🙊', '⏺️', '🤷🏿\\u200d♂️', '🤷🏾\\u200d♂️', '🏈', '🆘', '😵', '🌆', '🚕', '🤲🏽', '✏️', '🔊', '🗿', '🚏', '💪🏾', '🏂', '🔪', '🙏🏽', '😹', '🤦🏻\\u200d♀️', '🅿', '🅾', '®️', '🅾️', '💤', '🅰', '🅿️', '🇩🇪', '💰', '🔍', '🇳🇱', '👌🏻', '🇪🇦', '🔯', '🎊', '🤞🏻', '🛍', '💲', '🇫🇷', '🇪🇸', '💂\\u200d♀️', '🐸', '♀', '🗺', '⏰', '👈🏼', '🖖🏻', '🏎', '🔵', '🤦🏽\\u200d♂️', '🤳', '😸', '💒', '🛒', '🤝', '🌿', '🇺🇬', '🍆', '✌🏾', '♿', '👭', '🗳', '👥', '🔃', '🍑', '📅', '🏛', '💷', '🤐', '🤟', '👎🏽', '⛓️', '🍀', '⛪', '🇦🇺', '☯', '🦝', '♾', '🤏🏻', '🔷', '🕊', '🙅', '💁', '🤦🏿\\u200d♀', '🙁', '🤖', '🇨🇮', '💅🏻', '🇸🇴', '🇺🇿', '🤷🏾\\u200d♀️', '🎲', '👭🏾', '💅🏼', '🐠', '🛸', '🤘', '🌚', '🔄', '✌🏼', '🐓', '⚡', '💅🏾', '🌝', '🧍', '🤷🏻', '🦆', '🐤', '🐔', '🗑️', '🧚\\u200d♂️', '📘', '💞', '🇬🇭', '⚖', '🎟', '✝️', '⚽', '🏃', '☔', '👼', '🆒', '🍓', '⬆️', '✉', '🦾', '🅱', '🇫🇯', '🇲🇹', '🥖', '🍎', '🥗', '🏨', '✊🏿', '🌍', '🏊\\u200d♂️', '👙', '➖', '⚠️', '🥃', '🥩', '👌🏽', '🤞🏽', '⏱', '👩\\u200d👩\\u200d👧\\u200d👦', '🦧', '💆🏻\\u200d♀', '🇪🇺', '☑️', '🌴', '📽', '❣️', '🎸', '🤞', '🧒', '🤸\\u200d♂️', '🇲🇽', '🇻🇦', '🐘', '🎞️', '🥚', '🍒', '💢', '©', '👬', '🍵', '📆', '🇲🇻', '🇸🇨', '🇹🇷', '📰', '🔮', '🌙', '🤲🏼', '🎓', '🎭', '💅🏽', '📹', '📛', '💣', '🕸️', '🧨', '🟢', '🖥️', '🍞', '🥯', '👯\\u200d♂️', '🔨', '🥪', '💊', '🙋🏽\\u200d♀', '🐌', '🐞', '🕵', '📜', '🤦🏼\\u200d♀', '👨\\u200d👩\\u200d👦', '👨\\u200d👨\\u200d👧\\u200d👧', '🇵🇭', '🤰🏼', '🏔', '❓', '🇨🇴', '🎙️', '🧍🏽\\u200d♀️', '✔', '🤦\\u200d♀', '🇯🇵', '🕖', '🇮🇹', '🤘🏽', '🐽', '🙌🏻', '🚂', '🚗', '🦖', '👉🏻', '🤲', '📲', '🎾', '🐷', '🎁', '🔞', '🕸', '☹', '🏋🏾\\u200d♀️', '🤙🏾', '🆙', '🌋', '👿', '🤙🏼', '🏆', '🕤', '🎫', '🏏', '🇱🇰', '🧚🏾\\u200d♂️', '🍡', '🗳️', '🖥', '🆕', '📻', '🎚️', '🙏🏿', '🔹', '🇧🇴', '☮', '🕹️', '💭', '🦕', '🚮', '💿', '🧘🏽\\u200d♂️', '🎟️', '🍋', '🐭', '🧀', '🥤', '🎙', '🤦🏼', '✌🏽', '♟', '💦', '👅', '🇮🇪', '🤕', '🌼', '🔐', '🇹🇹', '🇲🇲', '🇹🇳', '👩\\u200d🦳', '🐢', '👩', '👩🏾', '🛑', '⚠', '☎️', '⚖️', '🦘', '🇮🇱', '🕵🏽\\u200d♂️', '💘', '🧍\\u200d♀', '🙆\\u200d♀️', '💌', '😴', '🤷🏾', '🇦🇲', '✖', '🤦🏾\\u200d♀', '👭🏻', '👬🏽', '🦦', '👇🏿', '🙌🏾', '🤦🏻\\u200d♀', '🗓', '👰', '🖕🏾', '🇨🇺', '🙅🏿\\u200d♂️', '🤒', '🇸🇱', '🇵🇹', '⏳', '🖐🏻', '📎', '🧍🏽', '🕺', '📧', '🎷', '🦜', '🙆🏾\\u200d♂️', '⚾', '☝🏾', '👐', '👉🏾', '👈🏾', '😗', '👩🏾\\u200d💻', '🤸🏻\\u200d♀️', '☝🏻', '☀', '🎨', '💄', '🥊', '🌧', '🥟', '🍱', '🍜', '🐾', '🧟', '💵', '🤥', '🖼', '🛤️', '🩸', '⬅️', '🐴', '🚭', '💁\\u200d♂️', '🤦🏿\\u200d♂️', '❎', '🚬', '💁🏼\\u200d♂️', '💁🏽\\u200d♂️', '💁🏾\\u200d♂️', '💁🏿\\u200d♂️', '🥶', '🐕\\u200d🦺', '⛱️', '🗡', '🍕', '🌭', '🙋\\u200d♂️', '🇨🇭', '🀄', '🖐️', '☯️', '🤷🏻\\u200d♂', '🌤', '🌦', '🏰', '💐', '🏃🏽\\u200d♀️', '⬛', '🇸🇳', '🇮🇴', '🛶', '🧘🏽\\u200d♂', '🍦', '🃏', '🏍', '🆗', '🌫️', '🏖️', '🥎', '👇🏾', '🧝\\u200d♀️', '⬆', '🥐', '⛲', '🥕', '🤷\\u200d♀', '☄️', '🖕🏽', '🪑', '🛐', '🏋🏻\\u200d♀️', '☠', '🏌️', '🌡️', '🗡️', '⚙️', '🇸🇹', '🇹🇯', '🔱', '👣', '🇩🇰', '🖋', '📨', '🐑', '🎀', '⏩', '🐙', '🙋\\u200d♀', '♠️', '🧙\\u200d♀️', '🌶️', '🕶️', '⛔', '☸️', '⏺', '🤦🏽', '🙋🏾\\u200d♀️', '🗝️', '🏃🏻', '🌷', '🤷🏿\\u200d♀️', '🐐', '📳', '👨\\u200d⚖️', '⛺', '🕌', '👏🏾', '🧔', '👌🏿', '🐜', '🇫🇮', '🏖', '🩱', '🙅\\u200d♀️', '🧚🏼', '🦃', '🤘🏼', '🇧🇷', '🇸🇭', '🇹🇼', '🎻', '✉️', '💧', '🌶', '🍝', '🧶', '💁🏻\\u200d♀️', '🥁', '👸🏻', '🌕', '♋', '🐨', '🍷', '🌪', '🧚', '🤦🏻\\u200d♂', '👨\\u200d⚕️', '🚢', '💼', '🐁', '🤸🏽\\u200d♀️', '👩\\u200d🎓', '👨\\u200d🎓', '🍩', '📊', '🙆🏻\\u200d♀️', '👼🏻', '🌮', '🇳🇴', '🇹🇭', '👉🏽', '🚪', '🏋\\u200d♂️', '💃🏼', '🦠', '🖌️', '🏴\\u200d☠', '🛩', '🕊️', '🧎🏻\\u200d♂', '👞', '👩🏻', '🧝🏽\\u200d♂️', '🦂', '🇷🇪', '🌱', '🤦🏽\\u200d♂', '🕋', '📽️', '🇱🇺', '👎🏻', '🧍🏻', '🌏', '🟠', '💁\\u200d♀️', '🇧🇲', '🥥', '🙅\\u200d♂️', '☪️', '🤜', '🤛', '☦️', '1⃣', '📉', '🇲🇾', '😼', '👋🏼', '🙋🏻\\u200d♂️', '🛹', '🇨🇬', '🇲🇨', '🇸🇾', '👆🏼', '👺', '🔙', '👨🏽\\u200d🦱', '👨🏾\\u200d🦱', '🇲🇩', '🇸🇬', '🐄', '🔎', '💁🏻', '🏃🏼\\u200d♀️', '🦀', '🏌️\\u200d♀️', '🦶', '🐲', '🤰🏽', '🙇\\u200d♂️', '🙇\\u200d♀️', '🏛️', '💑', '👩\\u200d❤️\\u200d👩', '🧘\\u200d♂️', '⛰️', '🍗', '🛫', '🌃', '🛬', '😾', '🎡', '🚷', '😽', '👨🏽', '🔅', '⛽', '💍', '🐊', '🧘', '💺', '🍼', '🇦🇪', '⌚', '🕺🏻', '💅🏿', '🇧🇩', '👩\\u200d❤️\\u200d💋\\u200d👩', '🤦🏾\\u200d♂', '🕯', '🧦', '🍉', '👦', '👧', '✈️', '🧿', '🍻', '🌥', '🇰🇪', '🇰🇬', '🇻🇨', '🇭🇲', '📴', '♊', '🎦', '🖖', '🤷🏻\\u200d♀', '🕰', '♠', '🙀', '🙆\\u200d♂️', '🇰🇷', '📮', '👋🏿', '📏', '🎈', '👨🏻\\u200d🦰', '🧎\\u200d♀️', '🧛\\u200d♂️', '🧱', '👨\\u200d👩\\u200d👧\\u200d👦', '✖️', '🤙🏻', '🕶', '🍺', '▶', '🇶🇦', '🔧', '🤦🏾\\u200d♀️', '👩🏽\\u200d🦽', '🤷🏽', '🏃🏻\\u200d♀', '🔕', '🇦🇬', '🇧🇳', '🇸🇦', '🤞🏼', '👟', '💃🏻', '🇳🇬', '🇵🇰', '🇰🇳', '🕵️\\u200d♂️', '🐦', '🙌🏼', '🧍🏼\\u200d♂️', '🧘\\u200d♀️', '🌽', '🏃\\u200d♀️', '🦸', '🦹', '🍫', '🇭🇰', '🙅🏼\\u200d♀️', '☁', '🇮🇶', '🌺', '🍇', '♨️', '👈🏻', '🇮🇩', '5⃣', '👤', '♉', '👕', '🇳🇪', '🇧🇱', '🧢', '👍🏿', '☢', '☢️', '💃🏾', '🦇', '🧛🏻\\u200d♀️', '👩🏿', '👴', '🧸', '👸', '🌬', '📙', '🧙\\u200d♂️', '🟥', '🟩', '🩲', '🤙🏽', '👩🏽', '🌲', '🇳🇦', '🇱🇮', '🇴🇲', '🇱🇨', '🧍\\u200d♂️', '⚜️', '❕', '🇵🇷', '🕉️', '👨\\u200d🚒', '🐏', '🌠', '🇺🇾', '👏🏿', '🇮🇳', '👩\\u200d✈️', '🖊', '🇨🇱', '🚉', '💴', '🧎🏾', '🍊', '📩', '👂', '⚒️', '👨🏻\\u200d💻', '👩🏼\\u200d💼', '🥋', '👉🏿', '👪', '👨\\u200d👩\\u200d👧', '👨\\u200d👩\\u200d👦\\u200d👦', '👨\\u200d👩\\u200d👧\\u200d👧', '👩\\u200d👩\\u200d👦', '👩\\u200d👩\\u200d👧', '👩\\u200d👩\\u200d👦\\u200d👦', '👩\\u200d👩\\u200d👧\\u200d👧', '👨\\u200d👨\\u200d👦', '👨\\u200d👨\\u200d👧', '👨\\u200d👨\\u200d👧\\u200d👦', '👨\\u200d👨\\u200d👦\\u200d👦', '👩\\u200d👦', '👩\\u200d👧', '👩\\u200d👧\\u200d👦', '👩\\u200d👦\\u200d👦', '👩\\u200d👧\\u200d👧', '👨\\u200d👦', '👨\\u200d👧', '👨\\u200d👧\\u200d👦', '👨\\u200d👦\\u200d👦', '👨\\u200d👧\\u200d👧', '👩🏿\\u200d🤝\\u200d👩🏻', '👨🏾\\u200d🔧', '🇲🇿', '🇳🇵', '🇵🇲', '🙅🏾\\u200d♂️', '🐃', '🕳️']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emoji_df = pd.read_csv('emoji.csv')\n",
    "emoji_list = emoji_df['emoji'].to_list()\n",
    "print(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2b2trans', '2lgbtqias', '2nite', 'aam', 'abbasi', 'abstrak', 'ade', 'adha', 'ael', 'ako', 'aku', 'aly', 'anarcha', 'anda', 'andalus', 'apa', 'as1anslgbt2', 'askip', 'assh0le', 'atonia', 'autsti', 'awwww', 'ay', 'bagus', 'bahan', 'bajan', 'basta', 'bearagon', 'behan', 'bencana', 'benda', 'berat', 'biden', 'bonang', 'bradbury', 'brancher', 'brexit', 'briton', 'broduil', 'cardia', 'carlin', 'cee', 'cfp', 'chakra', 'charle', 'chauvin', 'chay', 'chaya', 'chica', 'chucky', 'clawdeen', 'cleo', 'corruptor', 'covid', 'covid-19', 'cpa', 'crockett', 'cupcakke', 'daphshe', 'dari', 'dragulaura', 'elysia', 'etcnon', 'faham', 'fair1', 'familia', 'fascistspro', 'feminin', 'fication', 'fil', 'firefang', 'fishman', 'furrie', 'gaymo', 'gc', 'georgiafreedom', 'gette', 'goldwater', 'goli', 'gop', 'grat', 'grian', 'grt', 'gta', 'gue', 'hahaha', 'hairdress', 'hak', 'hamadani', 'hao', 'hoi', 'hud', 'ig', 'inb4', 'ini', 'iquity', 'isak', 'jure', 'jynx', 'kadhi', 'kaidan', 'kal', 'kalo', 'karlsberg', 'kashi', 'kenasia', 'kenisha', 'khutbah', 'koda', 'kulah', 'kulit', 'kurd', 'l3sbians', 'latifi', 'lesbianaclaw', 'lesbianlie', 'lgbtabc123xyz', 'lgbtiqa', 'lgbtliberty', 'lgbtq', 'lgbtq2', 'lgbtqia2', 'lgbt÷', 'lik', 'lipsky', 'localizer', 'looney', 'loverjupiter', 'lvt', 'ly', 'maga', 'mang', 'mannan', 'mansouri', 'masa', 'mau', 'mec', 'meio', 'merc', 'misandry', 'mitsuba', 'monogramme', 'mou', 'naam', 'nakong', 'nea', 'nep', 'nivea', 'nye', 'nyo', 'oda', 'oldsthese', 'ortho', 'pansim', 'pantalon', 'passionifyco', 'peasy', 'peele', 'pepperidge', 'polska', 'pongorma', 'positivi', 'potong', 'ppls', 'proship', 'proshpped0', 'psa', 'pst', 'purga', 'quebical', 'queerbaiting', 'quei', 'quigley', 'ransel', 'rated18', 'reis', 'ret', 'rogan', 'rostow', 'rp', 'rrow', 'sabina', 'sadr', 'sama', 'sandinista', 'saya', 'seung', 'shaykh', 'sisi', 'skłodowska', 'sok', 'sooo', 'soooo', 'spiegel', 'spina', 'stylo', 'su1c1de', 'tec', 'thanksgive', 'themis', 'titanbi', 'toon', 'utili', 'vag', 'vinny', 'wha', 'wickham', 'wtttffffff', 'ww2', 'xtian', 'yagi', 'zacatecas', 'zahra', '新人vtuber']\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('vocab.csv')\n",
    "vocab_list = vocab_df['word'].to_list()\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 1384 tokens\n"
     ]
    }
   ],
   "source": [
    "new_tokens = emoji_list + vocab_list\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    " # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-2b0b4ce672e3e08f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cac45d297ec48a9bc697d0fb91cf4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1381fbead958f0e2\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-8d5d2109338c8aa8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 59333\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 19778\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 19778\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize_function, batched=True, batch_size=None)\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['final', 'ly', 'my', 'first', 'politician', 'blocked', 'me', 'i', 'am', 'feeling', 'of', 'myself', 'r', 'ig', 'h', '##t', 'now', '😆']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('finally my first politician blocked me i am feeling of myself right now 😆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'you do not know how homosexuality is related to teaching propaganda to and showing them drag queen you really can not make that connection', 'label': 2, 'input_ids': [101, 2017, 2079, 2025, 2113, 2129, 15949, 2003, 3141, 2000, 4252, 17678, 8490, 31705, 2000, 1998, 4760, 2068, 8011, 3035, 2017, 2613, 31821, 2064, 2025, 2191, 2008, 4434, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded['train'][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(31906, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 3\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "         .to(device))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim='adamw_torch'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d7301225d0438a844afc2562be870c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 4.00 GiB total capacity; 2.73 GiB already allocated; 0 bytes free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\CZ4045---NLP\\BERT_model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                   compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                   args\u001b[39m=\u001b[39mtraining_args, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                   train_dataset\u001b[39m=\u001b[39mdataset_encoded[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                   eval_dataset\u001b[39m=\u001b[39mdataset_encoded[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                   tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/BERT_model.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2496\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2498\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2499\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2502\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2531\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2532\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2533\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1560\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1560\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1561\u001b[0m     input_ids,\n\u001b[0;32m   1562\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1563\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1564\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1565\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1566\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1567\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1568\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1569\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1570\u001b[0m )\n\u001b[0;32m   1572\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    612\u001b[0m         hidden_states,\n\u001b[0;32m    613\u001b[0m         attention_mask,\n\u001b[0;32m    614\u001b[0m         layer_head_mask,\n\u001b[0;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    617\u001b[0m         past_key_value,\n\u001b[0;32m    618\u001b[0m         output_attentions,\n\u001b[0;32m    619\u001b[0m     )\n\u001b[0;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    541\u001b[0m )\n\u001b[0;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m--> 551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[0;32m    552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:452\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    451\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 452\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\activations.py:56\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 4.00 GiB total capacity; 2.73 GiB already allocated; 0 bytes free; 2.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  args=training_args, \n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6fe2028bd84526aefa5ddb41cf25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06547744572162628,\n",
       " 'eval_accuracy': 0.9843710612553567,\n",
       " 'eval_runtime': 121.6135,\n",
       " 'eval_samples_per_second': 97.859,\n",
       " 'eval_steps_per_second': 6.118,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f4dac35224d53844635d1d3bbcc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0853798389434814,\n",
       " 'eval_accuracy': 0.7842198134610537,\n",
       " 'eval_runtime': 43.2866,\n",
       " 'eval_samples_per_second': 91.645,\n",
       " 'eval_steps_per_second': 5.729,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b49ebe8e4241189f55349e5b02f6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output=trainer.predict(dataset_encoded[\"test\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 556,   83,   92],\n",
       "       [  69, 1171,  246],\n",
       "       [  87,  279, 1384]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(dataset_encoded[\"test\"][\"label\"],predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased-finetuned'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "classifier = pipeline('text-classification', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"monkey pox is a hoax all the community hah 🤡 🏳️‍🌈\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {\n",
    "    'LABEL_0' : 'NEUTRAL',\n",
    "    'LABEL_1' : 'NEGATIVE',\n",
    "    'LABEL_2' : 'POSITIVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: monkey pox is a hoax all the community hah 🤡 🏳️‍🌈\n",
      "\n",
      "This sentence is classified with a NEGATIVE sentiment\n"
     ]
    }
   ],
   "source": [
    "c = classifier(sentence)\n",
    "print(f'\\nSentence: {sentence}')\n",
    "print(f\"\\nThis sentence is classified with a {classifier_dict[c[0]['label']]} sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
