{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1b67030b753636ac\n",
      "Reusing dataset csv (C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1b67030b753636ac\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3495a74cf2294d27b479d4eb095c26d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = './data/'\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': base_url+'bert_train.csv','validation': base_url+'bert_val.csv','test': base_url+'bert_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['™', '🙄', '✅', '💗', '😢', '🏳️\\u200d🌈', '😌', '🏳', '💀', '🌈', '👏🏻', '✨', '🌟', '🌹', '😕', '⭐', '😅', '🙏', '🐒', '🧐', '😭', '🖤', '🤧', '🤣', '🗃️', '🗓️', '➡️', '🔻', '🤤', '🌐', '😁', '❤️', '❤', '👏', '😍', '👽', '🚀', '📚', '🐱', '🌌', '😩', '🤷\\u200d♀️', '😬', '🦄', '🤸\\u200d♀️', '☀️', '👏🏼', '🔆', '🗣', '👀', '👍🏻', '🧑\\u200d🤝\\u200d🧑', '🎊', '☹️', '🥰', '💖', '🤭', '👎', '🎵', '🎶', '😊', '👌🏼', '🤟🏼', '💪🏼', '🔥', '🐻', '🐺', '😂', '♥️', '🤡', '💚', '💙', '💜', '🤔', '🍹', '🍸', '👌', '🏠', '🙃', '🚫', '👇', '😆', '😀', '🤪', '🤷🏼\\u200d♂️', '🥺', '📦', '🍃', '💨', '🌅', '🏝', '👬🏻', '🥵', '🧵', '😰', '💅', '👨\\u200d❤️\\u200d👨', '👨\\u200d❤️\\u200d💋\\u200d👨', '😔', '😇', '🤫', '🙏🏻', '🤷\\u200d♂️', '🇧🇿', '😞', '📣', '🤢', '😃', '😡', '💫', '💎', '🧛\\u200d♀️', '🥀', '🎹', '™️', '😺', '🤍', '💛', '🧡', '💕', '✋', '🗽', '🎃', '🇷🇺', '🤠', '💋', '🤯', '🎩', '🧁', '🍰', '✌️', '👨', '👨🏼', '🥴', '💔', '👍', '®', '😎', '🐕', '💩', '🐨', '😉', '🐧', '👍🏾', '😳', '🥳', '🤦\\u200d♀️', '👇🏽', '🔺', '🙌', '❌', '⭕', '😙', '🤦\\u200d♂️', '🤦\\u200d♂', '😑', '💥', '➡', '🔴', '🎮', '🚑', '⛑️', '😐', '🗣️', '💬', '😿', '😈', '😏', '💟', '▶️', '🌎', '🌞', '🤮', '💪🏻', '😒', '✊🏾', '☝', '🦉', '🙂', '📖', '🎉', '🤷🏻\\u200d♂️', '🅰️', '📌', '📷', '👍🏼', '❔', '👶🏻', '👧🏻', '🙈', '😘', '🐇', '🐿', '⚫', '👋', '😪', '💉', '☠️', '🇺🇲', '🤦', '😶', '💓', '☺', '🏳\\u200d🌈', '⚓', '🥂', '©️', '✊', '1️⃣', '5️⃣', '😝', '🤦🏻\\u200d♂️', '🇨🇦', '😣', '🚒', '✊🏽', '🤬', '🇺🇦', '🎥', '🍾', '🚨', '☝️', '🤷🏽\\u200d♀️', '🤨', '✍️', '💅🏻', '⬇', '🌻', '😷', '🦋', '🌳', '🦥', '🎤', '📝', '😄', '😓', '💁🏻\\u200d♂️', '🚩', '🤦🏼\\u200d♂️', '💝', '👑', '✌', '🎧', '🤦🏾\\u200d♂️', '🌸', '🖋', '📨', '📍', '🕐', '👮\\u200d♂️', '🚔', '👮\\u200d♀️', '🤷🏼\\u200d♀️', '🖕', '🤦🏽\\u200d♀️', '🕓', '👹', '🤩', '👉', '👈', '📼', '🛑', '☎', '✍🏻', '😠', '🐀', '🤜🏽', '🤛🏽', '🔗', '🏥', '🙋', '🗑', '👮🏻\\u200d♂️', '👷🏼\\u200d♀️', '🥱', '😻', '🇵🇸', '👾', '🍚', '❄️', '🇸🇩', '🇸🇲', '☺️', '🦊', '👻', '🤎', '😚', '❗', '😤', '😜', '🧑', '🎼', '🇦🇫', '👁', '👁️', '🙅🏻\\u200d♀️', '🕺🏽', '😨', '😥', '📺', '🎬', '📞', '🏌️\\u200d♂️', '⛳', '🍨', '🍔', '🍟', '💪', '👇🏻', '🇺🇸', '✊🏼', '🖕🏻', '😱', '🔫', '♣️', '🔖', '⬜', '📕', '👿', '🙏🏼', '😦', '🐯', '🦵🏿', '💃', '🦅', '🎂', '⬇️', '💸', '💯', '✊🏻', '🦈', '🏀', '👍🏽', '☮️', '😫', '🙋🏻\\u200d♀️', '👄', '🍂', '🧛\\u200d♀', '⚔', '😟', '🇷🇸', '🇬🇪', '⤵️', '😋', '🇧🇬', '🇱🇻', '🇷🇴', '🇸🇰', '🇨🇿', '🇵🇱', '🇱🇹', '🇪🇪', '🇭🇺', '🇸🇮', '🇧🇦', '🇲🇪', '🇦🇱', '🇲🇰', '🇽🇰', '🇭🇷', '🇬🇷', '✔️', '✌🏻', '🐝', '🎯', '🤷\\u200d♂', '📸', '🐥', '✍', '🙎🏼\\u200d♀️', '🙎🏾\\u200d♀', '👌🏿', '🙌🏿', '👆', '🧍\\u200d♀️', '🤷🏼', '💁🏾\\u200d♀️', '🐖', '♀️', '⚰️', '🦴', '🚦', '⛑', '🤚', '🏷', '🚌', '‼️', '⁉️', '😮', '🏉', '🤦🏼\\u200d♀️', '🌖', '🦁', '🍄', '🧠', '♥', '😯', '👊', '📈', '⚕️', '🇺🇳', '🤷', '🆓', '📱', '🛌', '🌊', '🐈', '👗', '🙉', '🏴\\u200d☠️', '🤦🏻', '🤷🏻\\u200d♀️', '⚔️', '👠', '😖', '🧻', '🐶', '😛', '🤙', '👇🏼', '🤓', '✡️', '👊🏽', '💪🏽', '🍿', '🙏🏾', '🕴', '🤚🏻', '🏡', '👩\\u200d🏫', '📢', '🤞', '👉🏼', '👏🏽', '⚡', '🍆', '🇷🇼', '🇸🇿', '🇻🇳', '☯️', '☕', '👋🏻', '🤷🏽\\u200d♂️', '🎞', '💡', '🤗', '🇨🇳', '⚪', '🤝', '🧍🏻\\u200d♂️', '🏁', '🏴', '🏳️', '🏊\\u200d♀️', '🚴\\u200d♀️', '🏅', '🥇', '🥈', '🥉', '👎🏾', '🐂', '🐍', '🏔️', '💻', '🏴\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f', '🇬🇧', '🐰', '🤑', '🔽', '🖼️', '🙊', '⏺️', '🤷🏿\\u200d♂️', '🔒', '🤷🏾\\u200d♂️', '🏈', '🆘', '😵', '🌆', '🚕', '🤲🏽', '✏️', '🏊🏽\\u200d♂️', '🔊', '🗿', '🚏', '💪🏾', '🏂', '🔪', '🙏🏽', '😹', '🤦🏻\\u200d♀️', '🅿', '🅾', '®️', '🅾️', '💤', '🅰', '🅿️', '🇩🇪', '💰', '🔍', '🇳🇱', '👌🏻', '🇪🇦', '🔯', '🤞🏻', '🛍', '💲', '🇫🇷', '🇪🇸', '💂\\u200d♀️', '🐸', '♀', '👸', '🗺', '⏰', '👈🏼', '🖖🏻', '🏎', '🔵', '🤦🏽\\u200d♂️', '🤳', '😸', '💒', '🛒', '🌿', '🇺🇬', '✌🏾', '♿', '👭', '🗳', '👥', '🔃', '🍑', '👋🏼', '📅', '🏛', '💷', '❓', '🤐', '🤟', '👎🏽', '⛓️', '🍀', '⛪', '🇦🇺', '☯', '🙅', '🦝', '♾', '🤏🏻', '🔷', '🕊', '💁', '🏋🏽\\u200d♂️', '🤦🏿\\u200d♀', '🙁', '🇨🇮', '🇸🇴', '🇺🇿', '🤷🏾\\u200d♀️', '🎲', '👭🏾', '💅🏼', '🐠', '🛸', '🤘', '🌚', '🔄', '✌🏼', '🐓', '💅🏾', '🌝', '🧍', '🤷🏻', '🦆', '🐤', '🐔', '🗑️', '🤖', '📘', '💞', '🇬🇭', '⚖', '🎟', '🙋🏾\\u200d♀️', '⚽', '🏃', '☔', '👼']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emoji_df = pd.read_csv('emoji.csv')\n",
    "emoji_list = emoji_df['emoji'].to_list()\n",
    "print(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2lgbtqias', '2nite', 'abbasi', 'ade', 'aku', 'atonia', 'awwww', 'bagus', 'benda', 'biden', 'bradbury', 'cee', 'chakra', 'charle', 'cosa', 'covid-19', 'cupcakke', 'elysia', 'etcnon', 'fair1', 'familia', 'fil', 'furrie', 'gc', 'georgiafreedom', 'goldwater', 'gop', 'grat', 'grt', 'gta', 'hak', 'ig', 'inb4', 'kadhi', 'kalo', 'karlsberg', 'koda', 'kulah', 'l3sbians', 'latifi', 'lgbt2', 'lgbtabc123xyz', 'lgbtq', 'lgbtq2', 'lgbtqia2', 'lgbt÷', 'looney', 'mau', 'nep', 'peele', 'ppls', 'proship', 'proshpped0', 'rated18', 'rogan', 'rostow', 'rp', 'sama', 'su1c1de', 'tabeta', 'titanbi', 'vag', 'vinny', 'visto', 'wickham', 'ww2', 'yagi', 'zahra', '新人vtuber']\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('vocab.csv')\n",
    "vocab_list = vocab_df['word'].to_list()\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 660 tokens\n"
     ]
    }
   ],
   "source": [
    "new_tokens = emoji_list + vocab_list\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    " # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1b67030b753636ac\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-461ca9cf3c71fb17.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9330e9fe4ad340c299d44921dc7367da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\user\\.cache\\huggingface\\datasets\\csv\\default-1b67030b753636ac\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-02872931dfb08b19.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11901\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3967\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3967\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize_function, batched=True, batch_size=None)\n",
    "dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finally', 'my', 'first', 'politician', 'blocked', 'me', 'i', 'am', 'feeling', 'of', 'myself', 'r', 'ig', 'h', '##t', 'now', '😆']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('finally my first politician blocked me i am feeling of myself right now 😆'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'finally my first politician blocked me i am feeling of myself right now 😆', 'label': 2, 'input_ids': [101, 2633, 2026, 2034, 3761, 8534, 2033, 1045, 2572, 3110, 1997, 2870, 1054, 31144, 1044, 2102, 2085, 30602, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_encoded['train'][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(31182, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 3\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "         .to(device))\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = len(dataset_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=5,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim='adamw_torch'\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa68afd1e174c6a99ad2767f3ffba6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6797, 'learning_rate': 1.6005376344086022e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b098c96b8c4e95b37304f803357b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5421019792556763, 'eval_accuracy': 0.7859843710612554, 'eval_runtime': 41.5766, 'eval_samples_per_second': 95.414, 'eval_steps_per_second': 5.965, 'epoch': 1.0}\n",
      "{'loss': 0.4205, 'learning_rate': 1.2010752688172046e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f265a84cd64a89a268064b5d767055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.517335057258606, 'eval_accuracy': 0.8099319384925636, 'eval_runtime': 41.4091, 'eval_samples_per_second': 95.8, 'eval_steps_per_second': 5.989, 'epoch': 2.0}\n",
      "{'loss': 0.2638, 'learning_rate': 8.016129032258066e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47200d416794d70b19102d0498265e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6146219968795776, 'eval_accuracy': 0.8109402571212503, 'eval_runtime': 41.396, 'eval_samples_per_second': 95.83, 'eval_steps_per_second': 5.991, 'epoch': 3.0}\n",
      "{'loss': 0.1753, 'learning_rate': 4.021505376344086e-06, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d296ad259cf94b8d8333a89d6588537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8216711282730103, 'eval_accuracy': 0.8008570708343836, 'eval_runtime': 41.3828, 'eval_samples_per_second': 95.861, 'eval_steps_per_second': 5.993, 'epoch': 4.0}\n",
      "{'loss': 0.1113, 'learning_rate': 2.688172043010753e-08, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb91cedc194ccb97f23bb855d8b808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9536471366882324, 'eval_accuracy': 0.8033778674061003, 'eval_runtime': 41.4516, 'eval_samples_per_second': 95.702, 'eval_steps_per_second': 5.983, 'epoch': 5.0}\n",
      "{'train_runtime': 2215.8245, 'train_samples_per_second': 26.855, 'train_steps_per_second': 1.679, 'train_loss': 0.3296963148820464, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3720, training_loss=0.3296963148820464, metrics={'train_runtime': 2215.8245, 'train_samples_per_second': 26.855, 'train_steps_per_second': 1.679, 'train_loss': 0.3296963148820464, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  args=training_args, \n",
    "                  train_dataset=dataset_encoded[\"train\"],\n",
    "                  eval_dataset=dataset_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6fe2028bd84526aefa5ddb41cf25ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.06547744572162628,\n",
       " 'eval_accuracy': 0.9843710612553567,\n",
       " 'eval_runtime': 121.6135,\n",
       " 'eval_samples_per_second': 97.859,\n",
       " 'eval_steps_per_second': 6.118,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f4dac35224d53844635d1d3bbcc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0853798389434814,\n",
       " 'eval_accuracy': 0.7842198134610537,\n",
       " 'eval_runtime': 43.2866,\n",
       " 'eval_samples_per_second': 91.645,\n",
       " 'eval_steps_per_second': 5.729,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset_encoded[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b49ebe8e4241189f55349e5b02f6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output=trainer.predict(dataset_encoded[\"test\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 556,   83,   92],\n",
       "       [  69, 1171,  246],\n",
       "       [  87,  279, 1384]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm=confusion_matrix(dataset_encoded[\"test\"][\"label\"],predictions)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased-finetuned'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "model_name = f\"{checkpoint}-finetuned\"\n",
    "classifier = pipeline('text-classification', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"monkey pox is a hoax all the community hah 🤡 🏳️‍🌈\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {\n",
    "    'LABEL_0' : 'NEUTRAL',\n",
    "    'LABEL_1' : 'NEGATIVE',\n",
    "    'LABEL_2' : 'POSITIVE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: monkey pox is a hoax all the community hah 🤡 🏳️‍🌈\n",
      "\n",
      "This sentence is classified with a NEGATIVE sentiment\n"
     ]
    }
   ],
   "source": [
    "c = classifier(sentence)\n",
    "print(f'\\nSentence: {sentence}')\n",
    "print(f\"\\nThis sentence is classified with a {classifier_dict[c[0]['label']]} sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
