{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1650\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it really is not but good to know you will bel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indeed i will let us take a teensy fraction of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actually its not not only have child drag been...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>government considering repeal of sodomy law al...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on a clear day you can see forever opening nig...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  it really is not but good to know you will bel...      1\n",
       "1  indeed i will let us take a teensy fraction of...      2\n",
       "2  actually its not not only have child drag been...      1\n",
       "3  government considering repeal of sodomy law al...      2\n",
       "4  on a clear day you can see forever opening nig...      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/dataset.csv\", usecols=['text','label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19835, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    8846\n",
       "1    7308\n",
       "0    3681\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed_val = 17 \n",
    "random.seed(seed_val) \n",
    "np.random.seed(seed_val) \n",
    "torch.manual_seed(seed_val) \n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['™', '🙄', '✅', '💗', '😢', '🏳️\\u200d🌈', '😌', '🏳', '💀', '🌈', '👏🏻', '✨', '🌟', '🌹', '😕', '⭐', '😅', '🙏', '🐒', '🧐', '😭', '🖤', '🤧', '🤣', '🗃️', '🗓️', '➡️', '🔻', '🤤', '🌐', '😁', '❤️', '❤', '👏', '😍', '👽', '🚀', '📚', '🐱', '🌌', '😩', '🤷\\u200d♀️', '😬', '🦄', '🤸\\u200d♀️', '☀️', '👏🏼', '🔆', '🗣', '👀', '👍🏻', '🧑\\u200d🤝\\u200d🧑', '🎊', '☹️', '🥰', '💖', '🤭', '👎', '🎵', '🎶', '😊', '👌🏼', '🤟🏼', '💪🏼', '🔥', '🐻', '🐺', '😂', '♥️', '🤡', '💚', '💙', '💜', '🤔', '🍹', '🍸', '👌', '🏠', '🙃', '🚫', '👇', '😆', '😀', '🤪', '🤷🏼\\u200d♂️', '🥺', '📦', '🍃', '💨', '🌅', '🏝', '👬🏻', '🥵', '🧵', '😰', '💅', '👨\\u200d❤️\\u200d👨', '👨\\u200d❤️\\u200d💋\\u200d👨', '😔', '😇', '🤫', '🙏🏻', '🤷\\u200d♂️', '🇧🇿', '😞', '📣', '🤢', '😃', '😡', '💫', '💎', '🧛\\u200d♀️', '🥀', '🎹', '™️', '😺', '🤍', '💛', '🧡', '💕', '✋', '🗽', '🎃', '🇷🇺', '🤠', '💋', '🤯', '🎩', '🧁', '🍰', '✌️', '👨', '👨🏼', '🥴', '💔', '👍', '®', '😎', '🐕', '💩', '🐨', '😉', '🐧', '👍🏾', '😳', '🥳', '🤦\\u200d♀️', '👇🏽', '🔺', '🙌', '❌', '⭕', '😙', '🤦\\u200d♂️', '🤦\\u200d♂', '😑', '💥', '➡', '🔴', '🎮', '🚑', '⛑️', '😐', '🗣️', '💬', '😿', '😈', '😏', '💟', '▶️', '🌎', '🌞', '🤮', '💪🏻', '😒', '✊🏾', '☝', '🦉', '🙂', '📖', '🎉', '🤷🏻\\u200d♂️', '🅰️', '📌', '📷', '👍🏼', '❔', '👶🏻', '👧🏻', '🙈', '😘', '🐇', '🐿', '⚫', '👋', '😪', '💉', '☠️', '🇺🇲', '🤦', '😶', '💓', '☺', '🏳\\u200d🌈', '⚓', '🥂', '©️', '✊', '1️⃣', '5️⃣', '😝', '🤦🏻\\u200d♂️', '🇨🇦', '😣', '🚒', '✊🏽', '🤬', '🇺🇦', '🎥', '🍾', '🚨', '☝️', '🤷🏽\\u200d♀️', '🤨', '✍️', '💅🏻', '⬇', '🌻', '😷', '🦋', '🌳', '🦥', '🎤', '📝', '😄', '😓', '💁🏻\\u200d♂️', '🚩', '🤦🏼\\u200d♂️', '💝', '👑', '✌', '🎧', '🤦🏾\\u200d♂️', '🌸', '🖋', '📨', '📍', '🕐', '👮\\u200d♂️', '🚔', '👮\\u200d♀️', '🤷🏼\\u200d♀️', '🖕', '🤦🏽\\u200d♀️', '🕓', '👹', '🤩', '👉', '👈', '📼', '🛑', '☎', '✍🏻', '😠', '🐀', '🤜🏽', '🤛🏽', '🔗', '🏥', '🙋', '🗑', '👮🏻\\u200d♂️', '👷🏼\\u200d♀️', '🥱', '😻', '🇵🇸', '👾', '🍚', '❄️', '🇸🇩', '🇸🇲', '☺️', '🦊', '👻', '🤎', '😚', '❗', '😤', '😜', '🧑', '🎼', '🇦🇫', '👁', '👁️', '🙅🏻\\u200d♀️', '🕺🏽', '😨', '😥', '📺', '🎬', '📞', '🏌️\\u200d♂️', '⛳', '🍨', '🍔', '🍟', '💪', '👇🏻', '🇺🇸', '✊🏼', '🖕🏻', '😱', '🔫', '♣️', '🔖', '⬜', '📕', '👿', '🙏🏼', '😦', '🐯', '🦵🏿', '💃', '🦅', '🎂', '⬇️', '💸', '💯', '✊🏻', '🦈', '🏀', '👍🏽', '☮️', '😫', '🙋🏻\\u200d♀️', '👄', '🍂', '🧛\\u200d♀', '⚔', '😟', '🇷🇸', '🇬🇪', '⤵️', '😋', '🇧🇬', '🇱🇻', '🇷🇴', '🇸🇰', '🇨🇿', '🇵🇱', '🇱🇹', '🇪🇪', '🇭🇺', '🇸🇮', '🇧🇦', '🇲🇪', '🇦🇱', '🇲🇰', '🇽🇰', '🇭🇷', '🇬🇷', '✔️', '✌🏻', '🐝', '🎯', '🤷\\u200d♂', '📸', '🐥', '✍', '🙎🏼\\u200d♀️', '🙎🏾\\u200d♀', '👌🏿', '🙌🏿', '👆', '🧍\\u200d♀️', '🤷🏼', '💁🏾\\u200d♀️', '🐖', '♀️', '⚰️', '🦴', '🚦', '⛑', '🤚', '🏷', '🚌', '‼️', '⁉️', '😮', '🏉', '🤦🏼\\u200d♀️', '🌖', '🦁', '🍄', '🧠', '♥', '😯', '👊', '📈', '⚕️', '🇺🇳', '🤷', '🆓', '📱', '🛌', '🌊', '🐈', '👗', '🙉', '🏴\\u200d☠️', '🤦🏻', '🤷🏻\\u200d♀️', '⚔️', '👠', '😖', '🧻', '🐶', '😛', '🤙', '👇🏼', '🤓', '✡️', '👊🏽', '💪🏽', '🍿', '🙏🏾', '🕴', '🤚🏻', '🏡', '👩\\u200d🏫', '📢', '🤞', '👉🏼', '👏🏽', '⚡', '🍆', '🇷🇼', '🇸🇿', '🇻🇳', '☯️', '☕', '👋🏻', '🤷🏽\\u200d♂️', '🎞', '💡', '🤗', '🇨🇳', '⚪', '🤝', '🧍🏻\\u200d♂️', '🏁', '🏴', '🏳️', '🏊\\u200d♀️', '🚴\\u200d♀️', '🏅', '🥇', '🥈', '🥉', '👎🏾', '🐂', '🐍', '🏔️', '💻', '🏴\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f', '🏴\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f', '🇬🇧', '🐰', '🤑', '🔽', '🖼️', '🙊', '⏺️', '🤷🏿\\u200d♂️', '🔒', '🤷🏾\\u200d♂️', '🏈', '🆘', '😵', '🌆', '🚕', '🤲🏽', '✏️', '🏊🏽\\u200d♂️', '🔊', '🗿', '🚏', '💪🏾', '🏂', '🔪', '🙏🏽', '😹', '🤦🏻\\u200d♀️', '🅿', '🅾', '®️', '🅾️', '💤', '🅰', '🅿️', '🇩🇪', '💰', '🔍', '🇳🇱', '👌🏻', '🇪🇦', '🔯', '🤞🏻', '🛍', '💲', '🇫🇷', '🇪🇸', '💂\\u200d♀️', '🐸', '♀', '👸', '🗺', '⏰', '👈🏼', '🖖🏻', '🏎', '🔵', '🤦🏽\\u200d♂️', '🤳', '😸', '💒', '🛒', '🌿', '🇺🇬', '✌🏾', '♿', '👭', '🗳', '👥', '🔃', '🍑', '👋🏼', '📅', '🏛', '💷', '❓', '🤐', '🤟', '👎🏽', '⛓️', '🍀', '⛪', '🇦🇺', '☯', '🙅', '🦝', '♾', '🤏🏻', '🔷', '🕊', '💁', '🏋🏽\\u200d♂️', '🤦🏿\\u200d♀', '🙁', '🇨🇮', '🇸🇴', '🇺🇿', '🤷🏾\\u200d♀️', '🎲', '👭🏾', '💅🏼', '🐠', '🛸', '🤘', '🌚', '🔄', '✌🏼', '🐓', '💅🏾', '🌝', '🧍', '🤷🏻', '🦆', '🐤', '🐔', '🗑️', '🤖', '📘', '💞', '🇬🇭', '⚖', '🎟', '🙋🏾\\u200d♀️', '⚽', '🏃', '☔', '👼']\n"
     ]
    }
   ],
   "source": [
    "emoji_df = pd.read_csv('emoji.csv')\n",
    "emoji_list = emoji_df['emoji'].to_list()\n",
    "print(emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2lgbtqias', '2nite', 'abbasi', 'ade', 'aku', 'atonia', 'awwww', 'bagus', 'benda', 'biden', 'bradbury', 'cee', 'chakra', 'charle', 'cosa', 'covid-19', 'cupcakke', 'elysia', 'etcnon', 'fair1', 'familia', 'fil', 'furrie', 'gc', 'georgiafreedom', 'goldwater', 'gop', 'grat', 'grt', 'gta', 'hak', 'ig', 'inb4', 'kadhi', 'kalo', 'karlsberg', 'koda', 'kulah', 'l3sbians', 'latifi', 'lgbt2', 'lgbtabc123xyz', 'lgbtq', 'lgbtq2', 'lgbtqia2', 'lgbt÷', 'looney', 'mau', 'nep', 'peele', 'ppls', 'proship', 'proshpped0', 'rated18', 'rogan', 'rostow', 'rp', 'sama', 'su1c1de', 'tabeta', 'titanbi', 'vag', 'vinny', 'visto', 'wickham', 'ww2', 'yagi', 'zahra', '新人vtuber']\n"
     ]
    }
   ],
   "source": [
    "vocab_df = pd.read_csv('vocab.csv')\n",
    "vocab_list = vocab_df['word'].to_list()\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have added 660 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(31182, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = emoji_list + vocab_list\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(new_tokens)\n",
    "print('We have added', num_added_toks, 'tokens')\n",
    " # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
    "bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "# encode text\n",
    "sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 14324, 2944, 14924, 4818, 102, 0], [101, 2057, 2097, 2986, 1011, 8694, 1037, 14324, 2944, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "print(sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids']).to(device)\n",
    "train_mask = torch.tensor(tokens_train['attention_mask']).to(device)\n",
    "train_y = torch.tensor(train_labels.tolist()).to(device)\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids']).to(device)\n",
    "val_mask = torch.tensor(tokens_val['attention_mask']).to(device)\n",
    "val_y = torch.tensor(val_labels.tolist()).to(device)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids']).to(device)\n",
    "test_mask = torch.tensor(tokens_test['attention_mask']).to(device)\n",
    "test_y = torch.tensor(test_labels.tolist()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.79588669 0.90478983 0.74741602]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    868.\n",
      "  Batch   100  of    868.\n",
      "  Batch   150  of    868.\n",
      "  Batch   200  of    868.\n",
      "  Batch   250  of    868.\n",
      "  Batch   300  of    868.\n",
      "  Batch   350  of    868.\n",
      "  Batch   400  of    868.\n",
      "  Batch   450  of    868.\n",
      "  Batch   500  of    868.\n",
      "  Batch   550  of    868.\n",
      "  Batch   600  of    868.\n",
      "  Batch   650  of    868.\n",
      "  Batch   700  of    868.\n",
      "  Batch   750  of    868.\n",
      "  Batch   800  of    868.\n",
      "  Batch   850  of    868.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    186.\n",
      "  Batch   100  of    186.\n",
      "  Batch   150  of    186.\n",
      "\n",
      "Training Loss: 1.117\n",
      "Validation Loss: 1.098\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    868.\n",
      "  Batch   100  of    868.\n",
      "  Batch   150  of    868.\n",
      "  Batch   200  of    868.\n",
      "  Batch   250  of    868.\n",
      "  Batch   300  of    868.\n",
      "  Batch   350  of    868.\n",
      "  Batch   400  of    868.\n",
      "  Batch   450  of    868.\n",
      "  Batch   500  of    868.\n",
      "  Batch   550  of    868.\n",
      "  Batch   600  of    868.\n",
      "  Batch   650  of    868.\n",
      "  Batch   700  of    868.\n",
      "  Batch   750  of    868.\n",
      "  Batch   800  of    868.\n",
      "  Batch   850  of    868.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    186.\n",
      "  Batch   100  of    186.\n",
      "  Batch   150  of    186.\n",
      "\n",
      "Training Loss: 1.098\n",
      "Validation Loss: 1.098\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    868.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\CZ4045---NLP\\new_bert.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Epoch \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{:}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#train model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m train_loss, _ \u001b[39m=\u001b[39m train()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#evaluate model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m valid_loss, _ \u001b[39m=\u001b[39m evaluate()\n",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\CZ4045---NLP\\new_bert.ipynb Cell 24\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# update parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# model predictions are stored on GPU. So, push it to CPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X34sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m preds\u001b[39m=\u001b[39mpreds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:362\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    360\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m(\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta1))\n\u001b[0;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m--> 362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39;49msqrt()\u001b[39m.\u001b[39;49madd_(group[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    364\u001b[0m step_size \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mcorrect_bias\u001b[39m\u001b[39m\"\u001b[39m]:  \u001b[39m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "import torch \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq[:1].to(device), test_mask[:1].to(device))\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Documents\\GitHub\\CZ4045---NLP\\new_bert.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# model's performance\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(preds, axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Documents/GitHub/CZ4045---NLP/new_bert.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(test_y, preds[:\u001b[39m1\u001b[39m]))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39margmax\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arts', '##y', '##7']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61d7d72412218704c5ba1799d65c7a83b08e24a9ca7847de9a479f6f426633e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
